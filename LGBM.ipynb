{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LGBM ",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a398e087d744587b6985d2991fbc148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8888b780e0aa4a7a95a72ef8b0071eb6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_395b053e56944ea69b6156a941cf921d",
              "IPY_MODEL_c08621aac9004cf2ad7b4e1654da7d53"
            ]
          }
        },
        "8888b780e0aa4a7a95a72ef8b0071eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "395b053e56944ea69b6156a941cf921d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_416b7dafe87e4ca9958089c559a12dd0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c65d9539835c438fbd069cf6ef37437c"
          }
        },
        "c08621aac9004cf2ad7b4e1654da7d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7dd53438e5fa4df8b237b8ecc556d2c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [43:30&lt;00:00, 522.14s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_70c07a485f9e48f9869edcf5a541770a"
          }
        },
        "416b7dafe87e4ca9958089c559a12dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c65d9539835c438fbd069cf6ef37437c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7dd53438e5fa4df8b237b8ecc556d2c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "70c07a485f9e48f9869edcf5a541770a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdpOZr7yF5rE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "773d6688-2a6f-41d4-e448-d0f83a48e659"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfFCcOZqGI17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/ML Project/Dataset/Original_Source')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ksjaiJ2vyZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "374fefe3-1792-49a8-e0ec-8c23b2827dd1"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "# from sklearn.preprocessing import train_test_split as split\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " alles_final.pkl\t submission14.csv      submission4.csv\n",
            " final2.csv\t\t submission15.csv      submission.csv\n",
            " final3.csv\t\t submission19.csv      submission_modparams1.csv\n",
            " final6.csv\t\t submission21.csv      submission_modparams3.csv\n",
            " final7.csv\t\t submission23.csv      submission_modparams.csv\n",
            " item_categories.csv\t'submission (2).csv'   submission_testmodified1.csv\n",
            " items.csv\t\t submission2.csv       submission_testmodified.csv\n",
            " sales_train.csv\t submission30.csv      test.csv\n",
            " sample_submission.csv\t submission31.csv      ver6_lr_stacking.csv\n",
            " shops.csv\t\t submission32.csv      XGBOOST_LSTM_LGBMsubmission1.csv\n",
            " submission11.csv\t'submission (3).csv'   XGBOOST_LSTM_LGBMsubmission.csv\n",
            " submission12.csv\t submission3.csv       XGBOOST_LSTM_submission.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWvNddLHNuRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = \"sales_train.csv\"\n",
        "test_path = \"test.csv\"\n",
        "items_path = \"items.csv\"\n",
        "shops_path = \"shops.csv\"\n",
        "item_cat_path = \"item_categories.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I07CYd2GOSL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "items = pd.read_csv(items_path)\n",
        "cats = pd.read_csv(item_cat_path)\n",
        "shops = pd.read_csv(shops_path)\n",
        "train = pd.read_csv( train_path )\n",
        "test = pd.read_csv( test_path )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhhYuIqVOiTg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "fe2afc24-c2b4-49e9-a7e6-7e7bca45ce04"
      },
      "source": [
        "#remove outliers\n",
        "plt.figure(figsize = (10,4))\n",
        "plt.xlim(-100, 3000)\n",
        "sns.boxplot( x= train.item_cnt_day )\n",
        "plt.figure( figsize = (10,4) )\n",
        "plt.xlim(train.item_price.min(), train.item_price.max())\n",
        "sns.boxplot( x = train.item_price )\n",
        "plt.show()\n",
        "\n",
        "print(train.shape[0])\n",
        "print(len( train[train.item_cnt_day >999 ] ))\n",
        "print( len(train[ train.item_cnt_day > 500  ]) )\n",
        "print(len(train[train.item_price >100000 ]))\n",
        "train = train[(train.item_price < 100000 )& (train.item_cnt_day < 1000)]\n",
        "print(train.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2935849\n",
            "2\n",
            "12\n",
            "1\n",
            "2935846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G3U0prIPXvC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c4f9ec59-2db1-45ab-ffd7-f9f078434502"
      },
      "source": [
        "#remove items with price < 0 and set items with count less that 1 to 0\n",
        "train = train[train.item_price > 0].reset_index(drop = True)\n",
        "train.loc[train.item_cnt_day < 1, \"item_cnt_day\"] = 0\n",
        "print(train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2935845, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trFKOnnDQ5-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.loc[train.shop_id == 0, \"shop_id\"] = 57\n",
        "test.loc[test.shop_id == 0 , \"shop_id\"] = 57\n",
        "train.loc[train.shop_id == 1, \"shop_id\"] = 58\n",
        "test.loc[test.shop_id == 1 , \"shop_id\"] = 58\n",
        "train.loc[train.shop_id == 11, \"shop_id\"] = 10\n",
        "test.loc[test.shop_id == 11, \"shop_id\"] = 10\n",
        "train.loc[train.shop_id == 40, \"shop_id\"] = 39\n",
        "test.loc[test.shop_id == 40, \"shop_id\"] = 39"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOBLaG_GQ-Ha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "94f36c62-bc9a-4a14-e699-8a29a0b73849"
      },
      "source": [
        "shops.loc[ shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"',\"shop_name\" ] = 'СергиевПосад ТЦ \"7Я\"'\n",
        "shops[\"city\"] = shops.shop_name.str.split(\" \").map( lambda x: x[0] )\n",
        "shops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )\n",
        "shops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n",
        "category = []\n",
        "for cat in shops.category.unique():\n",
        "    print(cat, len(shops[shops.category == cat]) )\n",
        "    if len(shops[shops.category == cat]) > 4:\n",
        "        category.append(cat)\n",
        "shops.category = shops.category.apply( lambda x: x if (x in category) else \"etc\" )\n",
        "for cat in shops.category.unique():\n",
        "    print(cat, len(shops[shops.category == cat]) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Орджоникидзе, 2\n",
            "ТЦ 28\n",
            "ТРК 5\n",
            "ТРЦ 11\n",
            "(Плехановская, 1\n",
            "Торговля 1\n",
            "ул. 2\n",
            "ЧС 1\n",
            "\"Распродажа\" 1\n",
            "МТРЦ 1\n",
            "Магазин 1\n",
            "ТК 5\n",
            "склад 1\n",
            "etc 11\n",
            "ТЦ 28\n",
            "ТРК 5\n",
            "ТРЦ 11\n",
            "ТК 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE2c2ORNRUDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "shops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\n",
        "shops[\"shop_city\"] = LabelEncoder().fit_transform( shops.city )\n",
        "shops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MgKOi4cRYZb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "25440cb6-e5d8-415a-c00e-baef8c511812"
      },
      "source": [
        "cats[\"type_code\"] = cats.item_category_name.apply( lambda x: x.split(\" \")[0] ).astype(str)\n",
        "cats.loc[ (cats.type_code == \"Игровые\")| (cats.type_code == \"Аксессуары\"), \"category\" ] = \"Игры\"\n",
        "cats.shape\n",
        "category = []\n",
        "for cat in cats.type_code.unique():\n",
        "    print(cat, len(cats[cats.type_code == cat]))\n",
        "    if len(cats[cats.type_code == cat]) > 4: \n",
        "        category.append( cat )\n",
        "cats.type_code = cats.type_code.apply(lambda x: x if (x in category) else \"etc\")\n",
        "for cat in cats.type_code.unique():\n",
        "    print(cat, len(cats[cats.type_code == cat]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PC 1\n",
            "Аксессуары 7\n",
            "Билеты 1\n",
            "Доставка 1\n",
            "Игровые 8\n",
            "Игры 14\n",
            "Карты 5\n",
            "Кино 5\n",
            "Книги 13\n",
            "Музыка 6\n",
            "Подарки 12\n",
            "Программы 6\n",
            "Служебные 2\n",
            "Чистые 2\n",
            "Элементы 1\n",
            "etc 8\n",
            "Аксессуары 7\n",
            "Игровые 8\n",
            "Игры 14\n",
            "Карты 5\n",
            "Кино 5\n",
            "Книги 13\n",
            "Музыка 6\n",
            "Подарки 12\n",
            "Программы 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvWelrd-RsLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cats.type_code = LabelEncoder().fit_transform(cats.type_code)\n",
        "cats[\"split\"] = cats.item_category_name.apply(lambda x: x.split(\"-\"))\n",
        "cats[\"subtype\"] = cats.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
        "cats[\"subtype_code\"] = LabelEncoder().fit_transform( cats[\"subtype\"] )\n",
        "cats = cats[[\"item_category_id\", \"subtype_code\", \"type_code\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41nGFs1gRwpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def name_correction(x):\n",
        "    x = x.lower()\n",
        "    x = x.partition('[')[0]\n",
        "    x = x.partition('(')[0]\n",
        "    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x)\n",
        "    x = x.replace('  ', ' ')\n",
        "    x = x.strip()\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Dot2ByRx8-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "f807c677-74e7-4f20-a4df-968d0531e02e"
      },
      "source": [
        "items = pd.read_csv(items_path)\n",
        "\n",
        "items[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\n",
        "items[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n",
        "\n",
        "items[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\n",
        "items[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\n",
        "items = items.fillna('0')\n",
        "\n",
        "items[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n",
        "items.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjGWDzYGR1BV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "items[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\n",
        "items.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\n",
        "items.loc[ items.type == \"\", \"type\"] = \"mac\"\n",
        "items.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\n",
        "items.loc[ (items.type == 'pc' )| (items.type == 'pс') | (items.type == \"pc\"), \"type\" ] = \"pc\"\n",
        "items.loc[ items.type == 'рs3' , \"type\"] = \"ps3\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcyuD_evR5aT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "73de2d8d-1a61-4245-d206-65981dd9e957"
      },
      "source": [
        "group_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\n",
        "print( group_sum.reset_index() )\n",
        "group_sum = group_sum.reset_index()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        type  item_id\n",
            "0          0    17661\n",
            "1        5c5        1\n",
            "2        5c7        2\n",
            "3        5f4        1\n",
            "4        6dv        1\n",
            "5        6jv        1\n",
            "6        6l6        1\n",
            "7    android        3\n",
            "8        hm3        1\n",
            "9        j72        4\n",
            "10       kf6        1\n",
            "11       kf7        1\n",
            "12       kg4        1\n",
            "13       mac       43\n",
            "14        pc     2628\n",
            "15        ps       79\n",
            "16       ps2        2\n",
            "17       ps3      611\n",
            "18       ps4      174\n",
            "19       psp      115\n",
            "20       s3v        1\n",
            "21       s4v        1\n",
            "22   xbox360      466\n",
            "23   xboxone      123\n",
            "24      англ        1\n",
            "25        рс       14\n",
            "26   русская        1\n",
            "27    только        9\n",
            "28     цифро        1\n",
            "29  цифровая      222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k76mM0LgR8qV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "7c50ea17-1dd3-4f9f-a9bc-09364a94651e"
      },
      "source": [
        "drop_cols = []\n",
        "for cat in group_sum.type.unique():\n",
        "#     print(group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0])\n",
        "    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n",
        "        drop_cols.append(cat)\n",
        "drop_cols"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['5c5',\n",
              " '5c7',\n",
              " '5f4',\n",
              " '6dv',\n",
              " '6jv',\n",
              " '6l6',\n",
              " 'android',\n",
              " 'hm3',\n",
              " 'j72',\n",
              " 'kf6',\n",
              " 'kf7',\n",
              " 'kg4',\n",
              " 'ps2',\n",
              " 's3v',\n",
              " 's4v',\n",
              " 'англ',\n",
              " 'рс',\n",
              " 'русская',\n",
              " 'только',\n",
              " 'цифро']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0tY083pSF2V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "201b5c82-335b-4406-cb6a-7cd2ef548773"
      },
      "source": [
        "items.name2 = items.name2.apply( lambda x: \"etc\" if (x in drop_cols) else x )\n",
        "items = items.drop([\"type\"], axis = 1)\n",
        "items.name2 = LabelEncoder().fit_transform(items.name2)\n",
        "items.name3 = LabelEncoder().fit_transform(items.name3)\n",
        "\n",
        "items.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)\n",
        "items.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_id</th>\n",
              "      <th>item_category_id</th>\n",
              "      <th>name2</th>\n",
              "      <th>name3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>1331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>64</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>1011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>1010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>1572</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   item_id  item_category_id  name2  name3\n",
              "0        0                40      4   1331\n",
              "1        1                76     64     42\n",
              "2        2                40      4   1011\n",
              "3        3                40      4   1010\n",
              "4        4                40      4   1572"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvqPgAxXSMEc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dafd37bc-be53-4817-d0af-924a403eca2e"
      },
      "source": [
        "from itertools import product\n",
        "import time\n",
        "ts = time.time()\n",
        "matrix = []\n",
        "cols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\n",
        "for i in range(34):\n",
        "    sales = train[train.date_block_num == i]\n",
        "    matrix.append( np.array(list( product( [i], sales.shop_id.unique(), sales.item_id.unique() ) ), dtype = np.int16) )\n",
        "\n",
        "matrix = pd.DataFrame( np.vstack(matrix), columns = cols )\n",
        "matrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\n",
        "matrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\n",
        "matrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\n",
        "matrix.sort_values( cols, inplace = True )\n",
        "time.time()- ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.15364122390747"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHAud9U2SSf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[\"revenue\"] = train[\"item_cnt_day\"] * train[\"item_price\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPVjPNs5SVED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5dcedf4-27a6-48c4-c125-b94fa9ad6ae4"
      },
      "source": [
        "ts = time.time()\n",
        "group = train.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\n",
        "group.columns = [\"item_cnt_month\"]\n",
        "group.reset_index( inplace = True)\n",
        "matrix = pd.merge( matrix, group, on = cols, how = \"left\" )\n",
        "matrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].fillna(0).clip(0,20).astype(np.float16)\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.4390645027160645"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaIhhKc8SX2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "57b1ee2b-3ef5-45c6-9a6f-5786cbfd32d2"
      },
      "source": [
        "test[\"date_block_num\"] = 34\n",
        "test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n",
        "test[\"shop_id\"] = test.shop_id.astype(np.int8)\n",
        "test[\"item_id\"] = test.item_id.astype(np.int16)\n",
        "ts = time.time()\n",
        "\n",
        "matrix = pd.concat([matrix, test.drop([\"ID\"],axis = 1)], ignore_index=True, sort=False, keys=cols)\n",
        "matrix.fillna( 0, inplace = True )\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07628464698791504"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv-XFjgrSdul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "55ede63b-cbec-4aab-b331-d645986dc337"
      },
      "source": [
        "ts = time.time()\n",
        "matrix = pd.merge( matrix, shops, on = [\"shop_id\"], how = \"left\" )\n",
        "matrix = pd.merge(matrix, items, on = [\"item_id\"], how = \"left\")\n",
        "matrix = pd.merge( matrix, cats, on = [\"item_category_id\"], how = \"left\" )\n",
        "matrix[\"shop_city\"] = matrix[\"shop_city\"].astype(np.int8)\n",
        "matrix[\"shop_category\"] = matrix[\"shop_category\"].astype(np.int8)\n",
        "matrix[\"item_category_id\"] = matrix[\"item_category_id\"].astype(np.int8)\n",
        "matrix[\"subtype_code\"] = matrix[\"subtype_code\"].astype(np.int8)\n",
        "matrix[\"name2\"] = matrix[\"name2\"].astype(np.int8)\n",
        "matrix[\"name3\"] = matrix[\"name3\"].astype(np.int16)\n",
        "matrix[\"type_code\"] = matrix[\"type_code\"].astype(np.int8)\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0602126121521"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCYaDaHxSg1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lag_feature( df,lags, cols ):\n",
        "    for col in cols:\n",
        "        print(col)\n",
        "        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\",col ]]\n",
        "        for i in lags:\n",
        "            shifted = tmp.copy()\n",
        "            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n",
        "            shifted.date_block_num = shifted.date_block_num + i\n",
        "            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tee9UGe8Sjba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "77dfe1a0-5f75-428a-9690-646ef2f07523"
      },
      "source": [
        "ts = time.time()\n",
        "\n",
        "matrix = lag_feature( matrix, [1,2,3], [\"item_cnt_month\"] )\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "item_cnt_month\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17.725797176361084"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hos6Cvb0SpbK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "71aead75-c17e-415b-d191-913845f7c3bb"
      },
      "source": [
        "ts = time.time()\n",
        "group = matrix.groupby( [\"date_block_num\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\n",
        "group.columns = [\"date_avg_item_cnt\"]\n",
        "group.reset_index(inplace = True)\n",
        "\n",
        "matrix = pd.merge(matrix, group, on = [\"date_block_num\"], how = \"left\")\n",
        "matrix.date_avg_item_cnt = matrix[\"date_avg_item_cnt\"].astype(np.float16)\n",
        "matrix = lag_feature( matrix, [1], [\"date_avg_item_cnt\"] )\n",
        "matrix.drop( [\"date_avg_item_cnt\"], axis = 1, inplace = True )\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date_avg_item_cnt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.510670185089111"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMRgUG4WSuMZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "02250e44-414e-4079-a6a9-2c7db4e3acc0"
      },
      "source": [
        "ts = time.time()\n",
        "group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\n",
        "group.columns = [ 'date_item_avg_item_cnt' ]\n",
        "group.reset_index(inplace=True)\n",
        "\n",
        "matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n",
        "matrix.date_item_avg_item_cnt = matrix['date_item_avg_item_cnt'].astype(np.float16)\n",
        "matrix = lag_feature(matrix, [1,2,3], ['date_item_avg_item_cnt'])\n",
        "matrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date_item_avg_item_cnt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22.415313482284546"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf9KSKoyS1tM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "54b19092-22ac-4721-a837-ab70131f486d"
      },
      "source": [
        "ts = time.time()\n",
        "group = matrix.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\n",
        "group.columns = [\"date_shop_avg_item_cnt\"]\n",
        "group.reset_index(inplace = True)\n",
        "\n",
        "matrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\"], how = \"left\")\n",
        "matrix.date_avg_item_cnt = matrix[\"date_shop_avg_item_cnt\"].astype(np.float16)\n",
        "matrix = lag_feature( matrix, [1,2,3], [\"date_shop_avg_item_cnt\"] )\n",
        "matrix.drop( [\"date_shop_avg_item_cnt\"], axis = 1, inplace = True )\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "date_shop_avg_item_cnt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23.045159101486206"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxey-dgFS8y4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "fe5acb5e-f0f2-4b87-f53e-a7791e42b754"
      },
      "source": [
        "ts = time.time()\n",
        "group = matrix.groupby( [\"date_block_num\",\"shop_id\",\"item_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\n",
        "group.columns = [\"date_shop_item_avg_item_cnt\"]\n",
        "group.reset_index(inplace = True)\n",
        "\n",
        "matrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\",\"item_id\"], how = \"left\")\n",
        "matrix.date_avg_item_cnt = matrix[\"date_shop_item_avg_item_cnt\"].astype(np.float16)\n",
        "matrix = lag_feature( matrix, [1,2,3], [\"date_shop_item_avg_item_cnt\"] )\n",
        "matrix.drop( [\"date_shop_item_avg_item_cnt\"], axis = 1, inplace = True )\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "date_shop_item_avg_item_cnt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugmAxX2nTBkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "48cb3592-1206-4996-a779-0579401ed984"
      },
      "source": [
        "ts = time.time()\n",
        "group = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\n",
        "group.columns = ['date_shop_subtype_avg_item_cnt']\n",
        "group.reset_index(inplace=True)\n",
        "\n",
        "matrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\n",
        "matrix.date_shop_subtype_avg_item_cnt = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\n",
        "matrix = lag_feature(matrix, [1], ['date_shop_subtype_avg_item_cnt'])\n",
        "matrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\n",
        "time.time() - ts\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date_shop_subtype_avg_item_cnt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.20657205581665"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByTYRFrQTK9Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "da979c0b-d4cf-478a-bec3-0746ff0bd694"
      },
      "source": [
        "ts = time.time()\n",
        "group = matrix.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': ['mean']})\n",
        "group.columns = ['date_city_avg_item_cnt']\n",
        "group.reset_index(inplace=True)\n",
        "\n",
        "matrix = pd.merge(matrix, group, on=['date_block_num', \"shop_city\"], how='left')\n",
        "matrix.date_city_avg_item_cnt = matrix['date_city_avg_item_cnt'].astype(np.float16)\n",
        "matrix = lag_feature(matrix, [1], ['date_city_avg_item_cnt'])\n",
        "matrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date_city_avg_item_cnt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.27081823348999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4sJG0ifTPPa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c2f6d5dc-eeb3-4a31-a10f-89e1e9aca723"
      },
      "source": [
        "ts = time.time()\n",
        "group = matrix.groupby(['date_block_num', 'item_id', 'shop_city']).agg({'item_cnt_month': ['mean']})\n",
        "group.columns = [ 'date_item_city_avg_item_cnt' ]\n",
        "group.reset_index(inplace=True)\n",
        "\n",
        "matrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\n",
        "matrix.date_item_city_avg_item_cnt = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\n",
        "matrix = lag_feature(matrix, [1], ['date_item_city_avg_item_cnt'])\n",
        "matrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date_item_city_avg_item_cnt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19.019996881484985"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1CUnRA-TSrt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "37dc597d-5f4f-4d6f-e3fe-07e9dca4fa33"
      },
      "source": [
        "ts = time.time()\n",
        "group = train.groupby( [\"item_id\"] ).agg({\"item_price\": [\"mean\"]})\n",
        "group.columns = [\"item_avg_item_price\"]\n",
        "group.reset_index(inplace = True)\n",
        "\n",
        "matrix = matrix.merge( group, on = [\"item_id\"], how = \"left\" )\n",
        "matrix[\"item_avg_item_price\"] = matrix.item_avg_item_price.astype(np.float16)\n",
        "\n",
        "\n",
        "group = train.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\n",
        "group.columns = [\"date_item_avg_item_price\"]\n",
        "group.reset_index(inplace = True)\n",
        "\n",
        "matrix = matrix.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\n",
        "matrix[\"date_item_avg_item_price\"] = matrix.date_item_avg_item_price.astype(np.float16)\n",
        "lags = [1, 2, 3]\n",
        "matrix = lag_feature( matrix, lags, [\"date_item_avg_item_price\"] )\n",
        "for i in lags:\n",
        "    matrix[\"delta_price_lag_\" + str(i) ] = (matrix[\"date_item_avg_item_price_lag_\" + str(i)]- matrix[\"item_avg_item_price\"] )/ matrix[\"item_avg_item_price\"]\n",
        "\n",
        "def select_trends(row) :\n",
        "    for i in lags:\n",
        "        if row[\"delta_price_lag_\" + str(i)]:\n",
        "            return row[\"delta_price_lag_\" + str(i)]\n",
        "    return 0\n",
        "\n",
        "matrix[\"delta_price_lag\"] = matrix.apply(select_trends, axis = 1)\n",
        "matrix[\"delta_price_lag\"] = matrix.delta_price_lag.astype( np.float16 )\n",
        "matrix[\"delta_price_lag\"].fillna( 0 ,inplace = True)\n",
        "\n",
        "features_to_drop = [\"item_avg_item_price\", \"date_item_avg_item_price\"]\n",
        "for i in lags:\n",
        "    features_to_drop.append(\"date_item_avg_item_price_lag_\" + str(i) )\n",
        "    features_to_drop.append(\"delta_price_lag_\" + str(i) )\n",
        "matrix.drop(features_to_drop, axis = 1, inplace = True)\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date_item_avg_item_price\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "305.2557158470154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcCSQccuV-Pr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "430baa3d-8b53-4286-c0c2-f84b862aad6d"
      },
      "source": [
        "\n",
        "ts = time.time()\n",
        "group = train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"revenue\": [\"sum\"] })\n",
        "group.columns = [\"date_shop_revenue\"]\n",
        "group.reset_index(inplace = True)\n",
        "\n",
        "matrix = matrix.merge( group , on = [\"date_block_num\", \"shop_id\"], how = \"left\" )\n",
        "matrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n",
        "\n",
        "group = group.groupby([\"shop_id\"]).agg({ \"date_block_num\":[\"mean\"] })\n",
        "group.columns = [\"shop_avg_revenue\"]\n",
        "group.reset_index(inplace = True )\n",
        "\n",
        "matrix = matrix.merge( group, on = [\"shop_id\"], how = \"left\" )\n",
        "matrix[\"shop_avg_revenue\"] = matrix.shop_avg_revenue.astype(np.float32)\n",
        "matrix[\"delta_revenue\"] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\n",
        "matrix[\"delta_revenue\"] = matrix[\"delta_revenue\"]. astype(np.float32)\n",
        "\n",
        "matrix = lag_feature(matrix, [1], [\"delta_revenue\"])\n",
        "matrix[\"delta_revenue_lag_1\"] = matrix[\"delta_revenue_lag_1\"].astype(np.float32)\n",
        "matrix.drop( [\"date_shop_revenue\", \"shop_avg_revenue\", \"delta_revenue\"] ,axis = 1, inplace = True)\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "delta_revenue\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.684904336929321"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QchJu_q8WMof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "99476314-0d07-47ec-ae94-2fac24cb292d"
      },
      "source": [
        "matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date_block_num</th>\n",
              "      <th>shop_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>item_cnt_month</th>\n",
              "      <th>shop_category</th>\n",
              "      <th>shop_city</th>\n",
              "      <th>item_category_id</th>\n",
              "      <th>name2</th>\n",
              "      <th>name3</th>\n",
              "      <th>subtype_code</th>\n",
              "      <th>type_code</th>\n",
              "      <th>item_cnt_month_lag_1</th>\n",
              "      <th>item_cnt_month_lag_2</th>\n",
              "      <th>item_cnt_month_lag_3</th>\n",
              "      <th>date_avg_item_cnt_lag_1</th>\n",
              "      <th>date_item_avg_item_cnt_lag_1</th>\n",
              "      <th>date_item_avg_item_cnt_lag_2</th>\n",
              "      <th>date_item_avg_item_cnt_lag_3</th>\n",
              "      <th>date_shop_avg_item_cnt_lag_1</th>\n",
              "      <th>date_shop_avg_item_cnt_lag_2</th>\n",
              "      <th>date_shop_avg_item_cnt_lag_3</th>\n",
              "      <th>date_shop_item_avg_item_cnt_lag_1</th>\n",
              "      <th>date_shop_item_avg_item_cnt_lag_2</th>\n",
              "      <th>date_shop_item_avg_item_cnt_lag_3</th>\n",
              "      <th>date_city_avg_item_cnt_lag_1</th>\n",
              "      <th>date_item_city_avg_item_cnt_lag_1</th>\n",
              "      <th>delta_price_lag</th>\n",
              "      <th>delta_revenue_lag_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>27</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>76</td>\n",
              "      <td>42</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>107</td>\n",
              "      <td>42</td>\n",
              "      <td>55</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>123</td>\n",
              "      <td>42</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11056272</th>\n",
              "      <td>34</td>\n",
              "      <td>45</td>\n",
              "      <td>18454</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>55</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.259033</td>\n",
              "      <td>0.045441</td>\n",
              "      <td>0.023254</td>\n",
              "      <td>0.071411</td>\n",
              "      <td>0.126709</td>\n",
              "      <td>0.129395</td>\n",
              "      <td>0.13916</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135376</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.475098</td>\n",
              "      <td>51790.574219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11056273</th>\n",
              "      <td>34</td>\n",
              "      <td>45</td>\n",
              "      <td>16188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.259033</td>\n",
              "      <td>0.022720</td>\n",
              "      <td>0.069763</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.126709</td>\n",
              "      <td>0.129395</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.135376</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.081116</td>\n",
              "      <td>51790.574219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11056274</th>\n",
              "      <td>34</td>\n",
              "      <td>45</td>\n",
              "      <td>15757</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>55</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.259033</td>\n",
              "      <td>0.113647</td>\n",
              "      <td>0.069763</td>\n",
              "      <td>0.095215</td>\n",
              "      <td>0.126709</td>\n",
              "      <td>0.129395</td>\n",
              "      <td>0.13916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135376</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.155884</td>\n",
              "      <td>51790.574219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11056275</th>\n",
              "      <td>34</td>\n",
              "      <td>45</td>\n",
              "      <td>19648</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>1367</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.259033</td>\n",
              "      <td>0.045441</td>\n",
              "      <td>0.069763</td>\n",
              "      <td>0.166626</td>\n",
              "      <td>0.126709</td>\n",
              "      <td>0.129395</td>\n",
              "      <td>0.13916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135376</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.091736</td>\n",
              "      <td>51790.574219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11056276</th>\n",
              "      <td>34</td>\n",
              "      <td>45</td>\n",
              "      <td>969</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>37</td>\n",
              "      <td>4</td>\n",
              "      <td>562</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.259033</td>\n",
              "      <td>0.068176</td>\n",
              "      <td>0.116272</td>\n",
              "      <td>0.023804</td>\n",
              "      <td>0.126709</td>\n",
              "      <td>0.129395</td>\n",
              "      <td>0.13916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135376</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.605957</td>\n",
              "      <td>51790.574219</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11056277 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          date_block_num  shop_id  ...  delta_price_lag  delta_revenue_lag_1\n",
              "0                      0        2  ...         0.000000                  NaN\n",
              "1                      0        2  ...         0.000000                  NaN\n",
              "2                      0        2  ...         0.000000                  NaN\n",
              "3                      0        2  ...         0.000000                  NaN\n",
              "4                      0        2  ...         0.000000                  NaN\n",
              "...                  ...      ...  ...              ...                  ...\n",
              "11056272              34       45  ...        -0.475098         51790.574219\n",
              "11056273              34       45  ...         0.081116         51790.574219\n",
              "11056274              34       45  ...         0.155884         51790.574219\n",
              "11056275              34       45  ...        -0.091736         51790.574219\n",
              "11056276              34       45  ...        -0.605957         51790.574219\n",
              "\n",
              "[11056277 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbn3utbMWOzh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "outputId": "f27f7c80-a5b1-4aae-8452-f9824d12f925"
      },
      "source": [
        "matrix.head().T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>date_block_num</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shop_id</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>item_id</th>\n",
              "      <td>19.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>32.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>item_cnt_month</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shop_category</th>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>shop_city</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>item_category_id</th>\n",
              "      <td>40.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>name2</th>\n",
              "      <td>4.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>123.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>name3</th>\n",
              "      <td>42.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>42.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>subtype_code</th>\n",
              "      <td>4.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>type_code</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>item_cnt_month_lag_1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>item_cnt_month_lag_2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>item_cnt_month_lag_3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_avg_item_cnt_lag_1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_item_avg_item_cnt_lag_1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_item_avg_item_cnt_lag_2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_item_avg_item_cnt_lag_3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_shop_avg_item_cnt_lag_1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_shop_avg_item_cnt_lag_2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_shop_avg_item_cnt_lag_3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_shop_item_avg_item_cnt_lag_1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_shop_item_avg_item_cnt_lag_2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_shop_item_avg_item_cnt_lag_3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_city_avg_item_cnt_lag_1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date_item_city_avg_item_cnt_lag_1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>delta_price_lag</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>delta_revenue_lag_1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      0     1      2      3     4\n",
              "date_block_num                      0.0   0.0    0.0    0.0   0.0\n",
              "shop_id                             2.0   2.0    2.0    2.0   2.0\n",
              "item_id                            19.0  27.0   28.0   29.0  32.0\n",
              "item_cnt_month                      0.0   1.0    0.0    0.0   0.0\n",
              "shop_category                       4.0   4.0    4.0    4.0   4.0\n",
              "shop_city                           0.0   0.0    0.0    0.0   0.0\n",
              "item_category_id                   40.0  19.0   30.0   23.0  40.0\n",
              "name2                               4.0  76.0  107.0  123.0   4.0\n",
              "name3                              42.0  42.0   42.0   42.0  42.0\n",
              "subtype_code                        4.0  10.0   55.0   16.0   4.0\n",
              "type_code                           5.0   3.0    3.0    3.0   5.0\n",
              "item_cnt_month_lag_1                NaN   NaN    NaN    NaN   NaN\n",
              "item_cnt_month_lag_2                NaN   NaN    NaN    NaN   NaN\n",
              "item_cnt_month_lag_3                NaN   NaN    NaN    NaN   NaN\n",
              "date_avg_item_cnt_lag_1             NaN   NaN    NaN    NaN   NaN\n",
              "date_item_avg_item_cnt_lag_1        NaN   NaN    NaN    NaN   NaN\n",
              "date_item_avg_item_cnt_lag_2        NaN   NaN    NaN    NaN   NaN\n",
              "date_item_avg_item_cnt_lag_3        NaN   NaN    NaN    NaN   NaN\n",
              "date_shop_avg_item_cnt_lag_1        NaN   NaN    NaN    NaN   NaN\n",
              "date_shop_avg_item_cnt_lag_2        NaN   NaN    NaN    NaN   NaN\n",
              "date_shop_avg_item_cnt_lag_3        NaN   NaN    NaN    NaN   NaN\n",
              "date_shop_item_avg_item_cnt_lag_1   NaN   NaN    NaN    NaN   NaN\n",
              "date_shop_item_avg_item_cnt_lag_2   NaN   NaN    NaN    NaN   NaN\n",
              "date_shop_item_avg_item_cnt_lag_3   NaN   NaN    NaN    NaN   NaN\n",
              "date_city_avg_item_cnt_lag_1        NaN   NaN    NaN    NaN   NaN\n",
              "date_item_city_avg_item_cnt_lag_1   NaN   NaN    NaN    NaN   NaN\n",
              "delta_price_lag                     0.0   0.0    0.0    0.0   0.0\n",
              "delta_revenue_lag_1                 NaN   NaN    NaN    NaN   NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsZCzC9RWZxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix[\"month\"] = matrix[\"date_block_num\"] % 12\n",
        "days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n",
        "matrix[\"days\"] = matrix[\"month\"].map(days).astype(np.int8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQLEErjcWgSH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "906ff8f6-fee6-45b0-9d46-ccfe04d75afa"
      },
      "source": [
        "ts = time.time()\n",
        "matrix[\"item_shop_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\",\"shop_id\"])[\"date_block_num\"].transform('min')\n",
        "matrix[\"item_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\"])[\"date_block_num\"].transform('min')\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.6975736618041992"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAfqC4U2Whhw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9490e9f6-29ec-4cd0-93d7-048dfdefcbd6"
      },
      "source": [
        "ts = time.time()\n",
        "matrix = matrix[matrix[\"date_block_num\"] > 3]\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9104199409484863"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo0oYYYtWl3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "import matplotlib.pylab as plt\n",
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize'] = 12, 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQCh_2iAWoZw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e083c72e-eb60-4cc5-8d58-f7673f006935"
      },
      "source": [
        "data = matrix.copy()\n",
        "del matrix\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5YulBn1Wqju",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b728d1d0-b044-48b1-b5f0-76e6a9b7ae12"
      },
      "source": [
        "data[data[\"date_block_num\"]==34].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(214200, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjdluH2oWtMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\n",
        "Y_train = data[data.date_block_num < 33]['item_cnt_month']\n",
        "X_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\n",
        "Y_valid = data[data.date_block_num == 33]['item_cnt_month']\n",
        "X_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcktPWZdblCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Function to reduce the DF size\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    #start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    #end_mem = df.memory_usage().sum() / 1024**2\n",
        "    #if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-e15aINbmXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = reduce_mem_usage(pd.DataFrame(X_train))\n",
        "Y_train = reduce_mem_usage(pd.DataFrame(Y_train))\n",
        "X_valid = reduce_mem_usage(pd.DataFrame(X_valid))\n",
        "Y_valid = reduce_mem_usage(pd.DataFrame(Y_valid))\n",
        "X_test = reduce_mem_usage(pd.DataFrame(X_test))\n",
        "y_train=Y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os7GW2JcWwMY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cb95f56-8f34-4280-88d5-1c01cef9ae36"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(214200, 31)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvNIB9z2WyOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del data\n",
        "\n",
        "\n",
        "\n",
        "gc.collect();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-KAkhAU666m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# categorical_features = (data.dtypes[data.dtypes.apply(str).str.startswith('int')].index).tolist()\n",
        "  categorical_features = ['item_category_id',\n",
        "  'shop_id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZeZ16Gh6OuJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6c117ce5-ca18-422e-b926-21e36a264bc3"
      },
      "source": [
        "!pip install lofo-importance\n",
        "!pip install pandarallel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lofo-importance in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from lofo-importance) (4.38.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lofo-importance) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lofo-importance) (1.18.3)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (from lofo-importance) (2.2.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from lofo-importance) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from lofo-importance) (1.0.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.3->lofo-importance) (0.14.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->lofo-importance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->lofo-importance) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->lofo-importance) (1.12.0)\n",
            "Requirement already satisfied: pandarallel in /usr/local/lib/python3.6/dist-packages (1.4.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from pandarallel) (0.3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCfhnzxsz4cn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "d1dcf19f-b6c9-4d5c-d488-5437257843da"
      },
      "source": [
        "!pip install bayesian-optimization\n",
        "!pip install scikit-optimize\n",
        "from bayes_opt import BayesianOptimization\n",
        "from skopt  import BayesSearchCV \n",
        "\n",
        "yT = Y_train['item_cnt_month'].values.astype(np.int32)\n",
        "\n",
        "def bayes_parameter_opt_lgb(X_train, yT, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=10000, output_process=False):\n",
        "    # prepare data\n",
        "    train_data = lgb.Dataset(data=X_train, label=yT, free_raw_data=False)\n",
        "    # parameters\n",
        "    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample,bagging_seed,feature_fraction_seed,bagging_freq):\n",
        "        params = {'application':'binary', 'metric':'auc'}\n",
        "        params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
        "        params[\"num_leaves\"] = int(round(num_leaves))\n",
        "        params['bagging_seed'] = int(round(bagging_seed))\n",
        "        params['bagging_freq'] = int(round(bagging_freq))\n",
        "        params['feature_fraction_seed'] = int(round(feature_fraction_seed))\n",
        "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
        "\n",
        "\n",
        "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
        "        params['max_depth'] = int(round(max_depth))\n",
        "        params['max_bin'] = int(round(max_depth))\n",
        "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
        "        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
        "        params['subsample'] = max(min(subsample, 1), 0)\n",
        "        \n",
        "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n",
        "        return max(cv_result['auc-mean'])\n",
        "     \n",
        "    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.006, 0.009),\n",
        "                                            'num_leaves': (200, 350),\n",
        "                                            'feature_fraction': (0.1, 0.9),\n",
        "                                            'bagging_fraction': (0.3, 1),\n",
        "                                            'max_depth': (5, 80),\n",
        "                                            'max_bin':(150,350),\n",
        "                                             'bagging_seed' : (0,1),\n",
        "                                            'min_data_in_leaf': (5, 10),\n",
        "                                            'min_sum_hessian_in_leaf':(0,10),\n",
        "                                            'bagging_freq':(0.5, 1),\n",
        "                                             'feature_fraction_seed':( 0,1),\n",
        "                                           'subsample': (0.01, 1.0)}, random_state=42)\n",
        "\n",
        "    # params = {'num_leaves': 256, 'max_depth': 19, 'max_bin': 250, 'n_estimators': 5000,\n",
        "    #           'bagging_freq': 1, 'bagging_fraction': 0.99, \n",
        "    #           'feature_fraction': 0.49446461478601994, 'min_data_in_leaf': 6, # 88\n",
        "    #           'learning_rate': 0.00762, 'num_threads': 16,\n",
        "    #           'min_sum_hessian_in_leaf': 6,\n",
        "    #           'random_state' : RAND_SEED,\n",
        "    #           'bagging_seed' : 0,\n",
        "    #           'feature_fraction_seed': 0,\n",
        "    #           'boost_from_average' : 'true',\n",
        "    #           'boost' : 'gbdt',\n",
        "    #           'metric' : 'rmse',\n",
        "    #           'verbose' : 1}\n",
        "    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
        "    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n",
        "    \n",
        "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
        "    \n",
        "    model_auc=[]\n",
        "    for model in range(len( lgbBO.res)):\n",
        "        model_auc.append(lgbBO.res[model]['target'])\n",
        "    \n",
        "    # return best parameters\n",
        "    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n",
        "\n",
        "opt_params = bayes_parameter_opt_lgb(X_train, yT, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.18.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (0.7.4)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (20.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.18.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "|   iter    |  target   | baggin... | baggin... | baggin... | featur... | featur... | learni... |  max_bin  | max_depth | min_da... | min_su... | num_le... | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8613  \u001b[0m | \u001b[0m 0.5622  \u001b[0m | \u001b[0m 0.9754  \u001b[0m | \u001b[0m 0.732   \u001b[0m | \u001b[0m 0.5789  \u001b[0m | \u001b[0m 0.156   \u001b[0m | \u001b[0m 0.006468\u001b[0m | \u001b[0m 161.6   \u001b[0m | \u001b[0m 69.96   \u001b[0m | \u001b[0m 8.006   \u001b[0m | \u001b[0m 7.081   \u001b[0m | \u001b[0m 203.1   \u001b[0m | \u001b[0m 0.9702  \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.858   \u001b[0m | \u001b[0m 0.8827  \u001b[0m | \u001b[0m 0.6062  \u001b[0m | \u001b[0m 0.1818  \u001b[0m | \u001b[0m 0.2467  \u001b[0m | \u001b[0m 0.3042  \u001b[0m | \u001b[0m 0.007574\u001b[0m | \u001b[0m 236.4   \u001b[0m | \u001b[0m 26.84   \u001b[0m | \u001b[0m 8.059   \u001b[0m | \u001b[0m 1.395   \u001b[0m | \u001b[0m 243.8   \u001b[0m | \u001b[0m 0.3727  \u001b[0m |\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8643  \u001b[0m | \u001b[95m 0.6192  \u001b[0m | \u001b[95m 0.8926  \u001b[0m | \u001b[95m 0.1997  \u001b[0m | \u001b[95m 0.5114  \u001b[0m | \u001b[95m 0.5924  \u001b[0m | \u001b[95m 0.006139\u001b[0m | \u001b[95m 271.5   \u001b[0m | \u001b[95m 17.79   \u001b[0m | \u001b[95m 5.325   \u001b[0m | \u001b[95m 9.489   \u001b[0m | \u001b[95m 344.8   \u001b[0m | \u001b[95m 0.8103  \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8624  \u001b[0m | \u001b[0m 0.5132  \u001b[0m | \u001b[0m 0.5488  \u001b[0m | \u001b[0m 0.6842  \u001b[0m | \u001b[0m 0.4521  \u001b[0m | \u001b[0m 0.122   \u001b[0m | \u001b[0m 0.007486\u001b[0m | \u001b[0m 156.9   \u001b[0m | \u001b[0m 73.2    \u001b[0m | \u001b[0m 6.294   \u001b[0m | \u001b[0m 6.625   \u001b[0m | \u001b[0m 246.8   \u001b[0m | \u001b[0m 0.5249  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8627  \u001b[0m | \u001b[0m 0.6827  \u001b[0m | \u001b[0m 0.5924  \u001b[0m | \u001b[0m 0.9696  \u001b[0m | \u001b[0m 0.7201  \u001b[0m | \u001b[0m 0.9395  \u001b[0m | \u001b[0m 0.008684\u001b[0m | \u001b[0m 269.6   \u001b[0m | \u001b[0m 74.14   \u001b[0m | \u001b[0m 5.442   \u001b[0m | \u001b[0m 1.96    \u001b[0m | \u001b[0m 206.8   \u001b[0m | \u001b[0m 0.3321  \u001b[0m |\n",
            "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.8647  \u001b[0m | \u001b[95m 0.4537  \u001b[0m | \u001b[95m 0.8742  \u001b[0m | \u001b[95m 0.09084 \u001b[0m | \u001b[95m 0.8527  \u001b[0m | \u001b[95m 0.1561  \u001b[0m | \u001b[95m 0.007373\u001b[0m | \u001b[95m 346.6   \u001b[0m | \u001b[95m 79.12   \u001b[0m | \u001b[95m 8.88    \u001b[0m | \u001b[95m 8.392   \u001b[0m | \u001b[95m 345.1   \u001b[0m | \u001b[95m 0.9282  \u001b[0m |\n",
            "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.8658  \u001b[0m | \u001b[95m 0.964   \u001b[0m | \u001b[95m 0.5979  \u001b[0m | \u001b[95m 0.5341  \u001b[0m | \u001b[95m 0.745   \u001b[0m | \u001b[95m 0.821   \u001b[0m | \u001b[95m 0.008785\u001b[0m | \u001b[95m 157.1   \u001b[0m | \u001b[95m 78.08   \u001b[0m | \u001b[95m 6.444   \u001b[0m | \u001b[95m 0.324   \u001b[0m | \u001b[95m 349.6   \u001b[0m | \u001b[95m 0.8881  \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8595  \u001b[0m | \u001b[0m 0.6392  \u001b[0m | \u001b[0m 0.7682  \u001b[0m | \u001b[0m 0.5945  \u001b[0m | \u001b[0m 0.2155  \u001b[0m | \u001b[0m 0.1625  \u001b[0m | \u001b[0m 0.008575\u001b[0m | \u001b[0m 349.1   \u001b[0m | \u001b[0m 76.29   \u001b[0m | \u001b[0m 9.586   \u001b[0m | \u001b[0m 9.057   \u001b[0m | \u001b[0m 349.7   \u001b[0m | \u001b[0m 0.4846  \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8602  \u001b[0m | \u001b[0m 0.4784  \u001b[0m | \u001b[0m 0.89    \u001b[0m | \u001b[0m 0.7524  \u001b[0m | \u001b[0m 0.3046  \u001b[0m | \u001b[0m 0.04859 \u001b[0m | \u001b[0m 0.007112\u001b[0m | \u001b[0m 151.6   \u001b[0m | \u001b[0m 77.84   \u001b[0m | \u001b[0m 6.725   \u001b[0m | \u001b[0m 1.512   \u001b[0m | \u001b[0m 346.5   \u001b[0m | \u001b[0m 0.7542  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8603  \u001b[0m | \u001b[0m 0.634   \u001b[0m | \u001b[0m 0.6456  \u001b[0m | \u001b[0m 0.5084  \u001b[0m | \u001b[0m 0.524   \u001b[0m | \u001b[0m 0.07157 \u001b[0m | \u001b[0m 0.008128\u001b[0m | \u001b[0m 152.2   \u001b[0m | \u001b[0m 10.34   \u001b[0m | \u001b[0m 8.399   \u001b[0m | \u001b[0m 4.807   \u001b[0m | \u001b[0m 349.7   \u001b[0m | \u001b[0m 0.9203  \u001b[0m |\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8522  \u001b[0m | \u001b[0m 0.6231  \u001b[0m | \u001b[0m 0.8708  \u001b[0m | \u001b[0m 0.7687  \u001b[0m | \u001b[0m 0.1545  \u001b[0m | \u001b[0m 0.2778  \u001b[0m | \u001b[0m 0.006673\u001b[0m | \u001b[0m 345.8   \u001b[0m | \u001b[0m 79.27   \u001b[0m | \u001b[0m 9.076   \u001b[0m | \u001b[0m 8.884   \u001b[0m | \u001b[0m 343.0   \u001b[0m | \u001b[0m 0.5637  \u001b[0m |\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8513  \u001b[0m | \u001b[0m 0.4603  \u001b[0m | \u001b[0m 0.8605  \u001b[0m | \u001b[0m 0.6246  \u001b[0m | \u001b[0m 0.2338  \u001b[0m | \u001b[0m 0.7314  \u001b[0m | \u001b[0m 0.006366\u001b[0m | \u001b[0m 343.5   \u001b[0m | \u001b[0m 7.802   \u001b[0m | \u001b[0m 5.306   \u001b[0m | \u001b[0m 1.741   \u001b[0m | \u001b[0m 209.3   \u001b[0m | \u001b[0m 0.1014  \u001b[0m |\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8633  \u001b[0m | \u001b[0m 0.6409  \u001b[0m | \u001b[0m 0.9486  \u001b[0m | \u001b[0m 0.03753 \u001b[0m | \u001b[0m 0.5603  \u001b[0m | \u001b[0m 0.8704  \u001b[0m | \u001b[0m 0.007781\u001b[0m | \u001b[0m 346.2   \u001b[0m | \u001b[0m 12.05   \u001b[0m | \u001b[0m 8.372   \u001b[0m | \u001b[0m 0.2238  \u001b[0m | \u001b[0m 338.5   \u001b[0m | \u001b[0m 0.4175  \u001b[0m |\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.856   \u001b[0m | \u001b[0m 0.6525  \u001b[0m | \u001b[0m 0.5701  \u001b[0m | \u001b[0m 0.4197  \u001b[0m | \u001b[0m 0.5294  \u001b[0m | \u001b[0m 0.6878  \u001b[0m | \u001b[0m 0.008116\u001b[0m | \u001b[0m 150.1   \u001b[0m | \u001b[0m 7.531   \u001b[0m | \u001b[0m 7.647   \u001b[0m | \u001b[0m 1.603   \u001b[0m | \u001b[0m 200.8   \u001b[0m | \u001b[0m 0.826   \u001b[0m |\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8638  \u001b[0m | \u001b[0m 0.5296  \u001b[0m | \u001b[0m 0.6326  \u001b[0m | \u001b[0m 0.6258  \u001b[0m | \u001b[0m 0.4319  \u001b[0m | \u001b[0m 0.2895  \u001b[0m | \u001b[0m 0.007355\u001b[0m | \u001b[0m 282.0   \u001b[0m | \u001b[0m 69.93   \u001b[0m | \u001b[0m 8.112   \u001b[0m | \u001b[0m 0.03688 \u001b[0m | \u001b[0m 349.3   \u001b[0m | \u001b[0m 0.3714  \u001b[0m |\n",
            "=========================================================================================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk-PiPiQz42G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "badc95ee-17a2-457a-895a-15ff207bdeac"
      },
      "source": [
        "opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\n",
        "opt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\n",
        "opt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\n",
        "opt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\n",
        "opt_params=opt_params[1]\n",
        "opt_params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bagging_fraction': 0.9639589902207968,\n",
              " 'bagging_freq': 0.5979448513249741,\n",
              " 'bagging_seed': 0.5341475068107697,\n",
              " 'feature_fraction': 0.7449874222314683,\n",
              " 'feature_fraction_seed': 0.8209781239992309,\n",
              " 'learning_rate': 0.008784729562129227,\n",
              " 'max_bin': 157,\n",
              " 'max_depth': 78,\n",
              " 'min_data_in_leaf': 6,\n",
              " 'min_sum_hessian_in_leaf': 0.3239855257428115,\n",
              " 'num_leaves': 350,\n",
              " 'subsample': 0.8880567272018562}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H70KYd7YXvkG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1a398e087d744587b6985d2991fbc148",
            "8888b780e0aa4a7a95a72ef8b0071eb6",
            "395b053e56944ea69b6156a941cf921d",
            "c08621aac9004cf2ad7b4e1654da7d53",
            "416b7dafe87e4ca9958089c559a12dd0",
            "c65d9539835c438fbd069cf6ef37437c",
            "7dd53438e5fa4df8b237b8ecc556d2c0",
            "70c07a485f9e48f9869edcf5a541770a"
          ]
        },
        "outputId": "248a8dcb-4ce3-48d0-e01d-5af1d46d07bb"
      },
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dateutil\n",
        "import datetime as dt\n",
        "from datetime import date\n",
        "from pandarallel import pandarallel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import gc\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "import statsmodels.api as sm\n",
        "from pylab import rcParams\n",
        "from itertools import permutations, product\n",
        "import time\n",
        "import lightgbm as lgb\n",
        "import random\n",
        "from lightgbm import LGBMRegressor\n",
        "from lofo import LOFOImportance, FLOFOImportance, Dataset, plot_importance\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import uniform as sp_uniform\n",
        "from sklearn.metrics import (mean_squared_log_error, mean_squared_error,\n",
        "                             mean_absolute_error)\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import warnings\n",
        "\n",
        "pandarallel.initialize()\n",
        "ts = time.time()\n",
        "\n",
        "kf = KFold(n_splits = 5, random_state=42)\n",
        "resu1 = 0\n",
        "impor1 = 0\n",
        "resu3_mae=0\n",
        "y_pred = 0\n",
        "y_final_test = 0\n",
        "stack_train = np.zeros([X_train.shape[0], ])\n",
        "models = []\n",
        "# params = {'num_leaves': 70,\n",
        "#           'objective': 'regression',\n",
        "#           'learning_rate': 0.1,\n",
        "#           \"boosting\": \"dart\",\n",
        "#           \"bagging_freq\": 5,\n",
        "#           \"bagging_fraction\": 0.1,\n",
        "#           \"feature_fraction\": 0.9,\n",
        "#           \"num_rounds\": 600,\n",
        "#           # 'max_depth':10\n",
        "#           }\n",
        "\n",
        "# RAND_SEED = 42\n",
        "\n",
        "# params = {'num_leaves': 2**8, #'max_depth': 19, 'max_bin': 107, #'n_estimators': 3747,\n",
        "#               'bagging_freq': 1, 'bagging_fraction': 0.7135681370918421, \n",
        "#               'feature_fraction': 0.49446461478601994, #'min_data_in_leaf': 2**8, # 88\n",
        "#               'learning_rate': 0.015980721586917768, 'num_threads': 4, \n",
        "#               'min_sum_hessian_in_leaf': 6,\n",
        "#               'random_state' : RAND_SEED,\n",
        "#               'bagging_seed' : RAND_SEED,\n",
        "#               'boost_from_average' : 'true',\n",
        "#               'boost' : 'dart',\n",
        "#               'metric' : 'rmse',\n",
        "#               'verbose' : 1}\n",
        "# params = {'num_leaves': 2**8, 'max_depth': 19, 'max_bin': 350, 'n_estimators': 3747,\n",
        "#               'bagging_freq': 1, 'bagging_fraction': 0.71, \n",
        "#               'feature_fraction': 0.49, 'min_data_in_leaf': 2**8, # 88\n",
        "#               'learning_rate': 0.015, 'num_threads': 4, \n",
        "#               'min_sum_hessian_in_leaf': 6,\n",
        "#               'random_state' : RAND_SEED,\n",
        "#               'bagging_seed' : RAND_SEED,\n",
        "#               'boost_from_average' : 'true',\n",
        "#               'boost' : 'gbdt',\n",
        "#               'metric' : 'rmse',\n",
        "#               'verbose' : 1}\n",
        "# params = {'num_leaves': 2**8, 'max_depth': 19, 'max_bin': 300, 'n_estimators': 3747,\n",
        "#               'bagging_freq': 1, 'bagging_fraction': 0.7135681370918421, \n",
        "#               'feature_fraction': 0.49446461478601994, 'min_data_in_leaf': 2**8, # 88\n",
        "#               'learning_rate': 0.015980721586917768, 'num_threads': 4, \n",
        "#               'min_sum_hessian_in_leaf': 6,\n",
        "#               'random_state' : RAND_SEED,\n",
        "#               'bagging_seed' : RAND_SEED,\n",
        "#               'boost_from_average' : 'true',\n",
        "#               'boost' : 'dart',\n",
        "#               'metric' : 'rmse',\n",
        "#               'verbose' : 1}\n",
        "\n",
        "# params = {'num_leaves': 2**8,  'max_bin': 270, #'n_estimators': 3747,\n",
        "#               'bagging_freq': 1, 'bagging_fraction': 0.7135681370918421, \n",
        "#               'feature_fraction': 0.49446461478601994, 'min_data_in_leaf': 2**8, # 88\n",
        "#               'learning_rate': 0.0015, 'num_threads': 4, \n",
        "#               'min_sum_hessian_in_leaf': 6,\n",
        "#               'random_state' : RAND_SEED,\n",
        "#               'bagging_seed' : RAND_SEED,\n",
        "#               'boost_from_average' : 'true',\n",
        "#               'boost' : 'gbdt',\n",
        "#               'metric' : 'rmse',\n",
        "#               'verbose' : 1}\n",
        "# params={\"early_stopping_rounds\":30, \n",
        "#             \"eval_metric\" : 'rmse', \n",
        "#             'learning_rate': 0.015980721586917768,\n",
        "#             'verbose': 100,\n",
        "#             # 'categorical_feature': 'auto',\n",
        "#            'num_leaves': random.randint(6, 50), \n",
        "#             #  'min_child_samples': sp_randint(100, 500), \n",
        "#             #  'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
        "#             #  'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
        "#             #  'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
        "#             #  'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
        "#             #  'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n",
        "# 'max_depth':-1, 'random_state':314, 'n_jobs':4, 'n_estimators':5000}\n",
        "# # params = { \"objective\" : \"regression\", \"metric\" : \"rmse\", 'n_estimators':10000, 'early_stopping_rounds':100,\n",
        "# #               \"num_leaves\" : 150, \"learning_rate\" : 0.015, \"bagging_fraction\" : 0.49,\n",
        "# #               \"feature_fraction\" : 0.3, \"bagging_seed\" : RAND_SEED,'boost' : 'dart',}\n",
        "    #try thiss\n",
        "RAND_SEED = 42\n",
        "# params = {'nthread': 4,\n",
        "#           'objective':'regression', \n",
        "#             'n_estimators': 7000,\n",
        "#             'learning_rate': .01,\n",
        "#             'num_leaves': 6,\n",
        "#           'max_bin':200,\n",
        "#           'bagging_fraction': 0.8,\n",
        "#           'bagging_freq': 4,\n",
        "#           'bagging_seed':8,\n",
        "#           'feature_fraction':0.2,\n",
        "#             'feature_fraction_seed':8,\n",
        "#           'min_sum_hessian_in_leaf': 11,\n",
        "#           'random_state':42,\n",
        "#             'verbose': -1,\n",
        "          \n",
        "#               'metric' : 'rmse',}\n",
        "# params = {'num_leaves': 256, 'max_depth': 19, 'max_bin': 250, 'n_estimators': 5000,\n",
        "#               'bagging_freq': 1, 'bagging_fraction': 0.99, \n",
        "#               'feature_fraction': 0.49446461478601994, 'min_data_in_leaf': 6, # 88\n",
        "#               'learning_rate': 0.00762, 'num_threads': 16,\n",
        "#               'min_sum_hessian_in_leaf': 6,\n",
        "#               'random_state' : RAND_SEED,\n",
        "#               'bagging_seed' : 0,\n",
        "#               'feature_fraction_seed': 0,\n",
        "#               'boost_from_average' : 'true',\n",
        "#               'boost' : 'gbdt',\n",
        "#               'metric' : 'rmse',\n",
        "#               'verbose' : 1}\n",
        "\n",
        "params={'bagging_fraction': 0.9639589902207968,\n",
        " 'bagging_freq': 1,\n",
        " 'bagging_seed': 0,\n",
        " 'feature_fraction':0.49446461478601994,\n",
        " 'feature_fraction_seed': 1,\n",
        " 'learning_rate': 0.00762,\n",
        " 'max_bin': 257,\n",
        " 'max_depth': 19,\n",
        " 'min_data_in_leaf': 6,\n",
        "\n",
        " 'num_leaves': 256,\n",
        "\n",
        " 'boost_from_average' : 'true',\n",
        "              'boost' : 'gbdt',\n",
        "              'metric' : 'rmse',\n",
        "              'verbose' : 1,'n_estimators': 5000, 'random_state' : RAND_SEED}\n",
        "# {'num_leaves': 2**8, #'max_depth': 19, \n",
        "#           'max_bin': 250, \n",
        "#           'n_estimators': 5000,\n",
        "#               'bagging_freq': 1, 'bagging_fraction':  0.9639589902207968, \n",
        "#               'feature_fraction': 0.49446461478601994, #'min_data_in_leaf': 2**8, # 88\n",
        "#               'learning_rate': 0.0075,\n",
        "#           'min_data_in_leaf': 5,\n",
        "#                'num_threads': 16, \n",
        "#                 #'device' : 'gpu',\n",
        "#               'min_sum_hessian_in_leaf': 6,\n",
        "#               'random_state' : RAND_SEED,\n",
        "#               'bagging_seed' : 0,\n",
        "#               'boost_from_average' : 'true',\n",
        "#               'metric' : 'rmse',\n",
        "#               'verbose' : 1}\n",
        "# # params = {\n",
        "\n",
        "#     'boosting_type': 'gbdt',\n",
        "#     'objective': 'regression',\n",
        "#     'learning_rate': 0.035,\n",
        "#     'metric': 'rmse',\n",
        "#     'is_training_metric' : True,\n",
        "#     'tree_leaner': 'data',\n",
        "#     'feature_fraction_seed' : 42,\n",
        "#     'verbose':0,\n",
        "#     'feature_fraction':0.85,\n",
        "#     'bagging_fraction': 0.45,\n",
        "#     'bagging_freq':7,\n",
        "#     # 'bagging_seed': 42,\n",
        "#     'l2_lambda':14,\n",
        "#     #'l1_lambda': 16,\n",
        "#     'num_leaves': 1024,\n",
        "#     'min_data_in_leaf':40,\n",
        "#     'min_gain_to_split':1,\n",
        "#     # 'max_bin' :300\n",
        "# }\n",
        "          \n",
        "for train_index, test_index in tqdm(list(kf.split(X_train, y_train))):\n",
        "    X_train2 = X_train.iloc[train_index, :]\n",
        "    y_train2 = y_train.iloc[train_index]\n",
        "    X_test2 = X_train.iloc[test_index, :]\n",
        "    y_test2 = y_train.iloc[test_index]\n",
        "    \n",
        "    d_training = lgb.Dataset(X_train2, label=y_train2,\n",
        "                             categorical_feature=categorical_features,\n",
        "                             free_raw_data=False)\n",
        "    d_test = lgb.Dataset(X_test2, label=y_test2,\n",
        "                         categorical_feature=categorical_features,\n",
        "                         free_raw_data=False)\n",
        "    \n",
        "    clf = lgb.train(params, train_set=d_training, num_boost_round=1000,\n",
        "                      valid_sets=[d_training, d_test], verbose_eval=25,\n",
        "                      early_stopping_rounds=100\n",
        "                    )\n",
        "\n",
        "    models.append(clf)\n",
        "    temp_predict = clf.predict(X_test2, num_iteration=clf.best_iteration)\n",
        "    stack_train[test_index] = temp_predict\n",
        "\n",
        "    y_pred += clf.predict(X_test, num_iteration=clf.best_iteration)/5\n",
        "    y_final_test += clf.predict(X_test, num_iteration=clf.best_iteration)/5\n",
        "    mse = mean_squared_error(y_test2, temp_predict)\n",
        "    mae = mean_absolute_error(y_test2, temp_predict)\n",
        "\n",
        "    resu1 += mse/5\n",
        "    resu3_mae += mae/5\n",
        "    impor1 += clf.feature_importance()/5\n",
        "    del X_train2, y_train2, X_test2, y_test2; gc.collect()\n",
        "\n",
        "print('xxxxxxxxx',resu1,resu3_mae)\n",
        "# model = lgb.LGBMRegressor(**params)\n",
        "\n",
        "\n",
        "# # model = XGBRegressor(\n",
        "# #     max_depth=10,\n",
        "# #     n_estimators=1000,\n",
        "# #     min_child_weight=0.5, \n",
        "# #     colsample_bytree=0.8, \n",
        "# #     subsample=0.8, \n",
        "# #     eta=0.1,\n",
        "# #      tree_method='gpu_hist',\n",
        "# #     seed=42)\n",
        "# # X_valid=X_train.iloc[train_index, :]\n",
        "# model.fit(\n",
        "#     X_train, \n",
        "#     Y_train, \n",
        "\n",
        "#     eval_metric=\"rmse\", \n",
        "#     eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
        "#     verbose=True, )\n",
        "#     # early_stopping_rounds = 50)\n",
        "\n",
        "time.time() - ts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Pandarallel will run on 40 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a398e087d744587b6985d2991fbc148",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
            "  warnings.warn('Using categorical_feature in Dataset.')\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
            "  warnings.warn('categorical_feature in param dict is overridden.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[25]\ttraining's rmse: 1.1391\tvalid_1's rmse: 1.09294\n",
            "[50]\ttraining's rmse: 1.06524\tvalid_1's rmse: 1.03678\n",
            "[75]\ttraining's rmse: 1.00635\tvalid_1's rmse: 0.993125\n",
            "[100]\ttraining's rmse: 0.961353\tvalid_1's rmse: 0.960745\n",
            "[125]\ttraining's rmse: 0.927024\tvalid_1's rmse: 0.936818\n",
            "[150]\ttraining's rmse: 0.900496\tvalid_1's rmse: 0.918813\n",
            "[175]\ttraining's rmse: 0.87907\tvalid_1's rmse: 0.904958\n",
            "[200]\ttraining's rmse: 0.862032\tvalid_1's rmse: 0.894309\n",
            "[225]\ttraining's rmse: 0.84823\tvalid_1's rmse: 0.886468\n",
            "[250]\ttraining's rmse: 0.836825\tvalid_1's rmse: 0.879867\n",
            "[275]\ttraining's rmse: 0.827249\tvalid_1's rmse: 0.874897\n",
            "[300]\ttraining's rmse: 0.818981\tvalid_1's rmse: 0.871184\n",
            "[325]\ttraining's rmse: 0.812251\tvalid_1's rmse: 0.867995\n",
            "[350]\ttraining's rmse: 0.806061\tvalid_1's rmse: 0.865454\n",
            "[375]\ttraining's rmse: 0.800818\tvalid_1's rmse: 0.863332\n",
            "[400]\ttraining's rmse: 0.795929\tvalid_1's rmse: 0.861577\n",
            "[425]\ttraining's rmse: 0.791341\tvalid_1's rmse: 0.859919\n",
            "[450]\ttraining's rmse: 0.786831\tvalid_1's rmse: 0.858663\n",
            "[475]\ttraining's rmse: 0.782937\tvalid_1's rmse: 0.857366\n",
            "[500]\ttraining's rmse: 0.779452\tvalid_1's rmse: 0.856355\n",
            "[525]\ttraining's rmse: 0.776082\tvalid_1's rmse: 0.855377\n",
            "[550]\ttraining's rmse: 0.772952\tvalid_1's rmse: 0.854508\n",
            "[575]\ttraining's rmse: 0.770259\tvalid_1's rmse: 0.853753\n",
            "[600]\ttraining's rmse: 0.767632\tvalid_1's rmse: 0.853089\n",
            "[625]\ttraining's rmse: 0.765094\tvalid_1's rmse: 0.852432\n",
            "[650]\ttraining's rmse: 0.762556\tvalid_1's rmse: 0.851885\n",
            "[675]\ttraining's rmse: 0.760195\tvalid_1's rmse: 0.851421\n",
            "[700]\ttraining's rmse: 0.757966\tvalid_1's rmse: 0.850869\n",
            "[725]\ttraining's rmse: 0.755956\tvalid_1's rmse: 0.850519\n",
            "[750]\ttraining's rmse: 0.754059\tvalid_1's rmse: 0.85016\n",
            "[775]\ttraining's rmse: 0.752129\tvalid_1's rmse: 0.849707\n",
            "[800]\ttraining's rmse: 0.750175\tvalid_1's rmse: 0.849387\n",
            "[825]\ttraining's rmse: 0.748419\tvalid_1's rmse: 0.849103\n",
            "[850]\ttraining's rmse: 0.746702\tvalid_1's rmse: 0.848805\n",
            "[875]\ttraining's rmse: 0.745197\tvalid_1's rmse: 0.848348\n",
            "[900]\ttraining's rmse: 0.743689\tvalid_1's rmse: 0.848026\n",
            "[925]\ttraining's rmse: 0.742299\tvalid_1's rmse: 0.847807\n",
            "[950]\ttraining's rmse: 0.740911\tvalid_1's rmse: 0.847553\n",
            "[975]\ttraining's rmse: 0.739585\tvalid_1's rmse: 0.847179\n",
            "[1000]\ttraining's rmse: 0.738319\tvalid_1's rmse: 0.846939\n",
            "[1025]\ttraining's rmse: 0.737105\tvalid_1's rmse: 0.846698\n",
            "[1050]\ttraining's rmse: 0.735722\tvalid_1's rmse: 0.846383\n",
            "[1075]\ttraining's rmse: 0.734522\tvalid_1's rmse: 0.846011\n",
            "[1100]\ttraining's rmse: 0.733319\tvalid_1's rmse: 0.845788\n",
            "[1125]\ttraining's rmse: 0.7321\tvalid_1's rmse: 0.845555\n",
            "[1150]\ttraining's rmse: 0.730918\tvalid_1's rmse: 0.845319\n",
            "[1175]\ttraining's rmse: 0.729705\tvalid_1's rmse: 0.845091\n",
            "[1200]\ttraining's rmse: 0.728636\tvalid_1's rmse: 0.844963\n",
            "[1225]\ttraining's rmse: 0.727683\tvalid_1's rmse: 0.844801\n",
            "[1250]\ttraining's rmse: 0.726597\tvalid_1's rmse: 0.844462\n",
            "[1275]\ttraining's rmse: 0.725394\tvalid_1's rmse: 0.844186\n",
            "[1300]\ttraining's rmse: 0.724498\tvalid_1's rmse: 0.844071\n",
            "[1325]\ttraining's rmse: 0.723365\tvalid_1's rmse: 0.84389\n",
            "[1350]\ttraining's rmse: 0.722376\tvalid_1's rmse: 0.843766\n",
            "[1375]\ttraining's rmse: 0.721283\tvalid_1's rmse: 0.843613\n",
            "[1400]\ttraining's rmse: 0.720331\tvalid_1's rmse: 0.843426\n",
            "[1425]\ttraining's rmse: 0.719454\tvalid_1's rmse: 0.843326\n",
            "[1450]\ttraining's rmse: 0.718483\tvalid_1's rmse: 0.843148\n",
            "[1475]\ttraining's rmse: 0.717475\tvalid_1's rmse: 0.843016\n",
            "[1500]\ttraining's rmse: 0.716507\tvalid_1's rmse: 0.842846\n",
            "[1525]\ttraining's rmse: 0.715709\tvalid_1's rmse: 0.842695\n",
            "[1550]\ttraining's rmse: 0.714787\tvalid_1's rmse: 0.842584\n",
            "[1575]\ttraining's rmse: 0.713736\tvalid_1's rmse: 0.842366\n",
            "[1600]\ttraining's rmse: 0.712923\tvalid_1's rmse: 0.842282\n",
            "[1625]\ttraining's rmse: 0.712045\tvalid_1's rmse: 0.842141\n",
            "[1650]\ttraining's rmse: 0.711216\tvalid_1's rmse: 0.841968\n",
            "[1675]\ttraining's rmse: 0.710343\tvalid_1's rmse: 0.841847\n",
            "[1700]\ttraining's rmse: 0.70942\tvalid_1's rmse: 0.84175\n",
            "[1725]\ttraining's rmse: 0.708566\tvalid_1's rmse: 0.841669\n",
            "[1750]\ttraining's rmse: 0.707838\tvalid_1's rmse: 0.841581\n",
            "[1775]\ttraining's rmse: 0.70689\tvalid_1's rmse: 0.841401\n",
            "[1800]\ttraining's rmse: 0.705924\tvalid_1's rmse: 0.84115\n",
            "[1825]\ttraining's rmse: 0.705147\tvalid_1's rmse: 0.841032\n",
            "[1850]\ttraining's rmse: 0.704458\tvalid_1's rmse: 0.840971\n",
            "[1875]\ttraining's rmse: 0.703571\tvalid_1's rmse: 0.8409\n",
            "[1900]\ttraining's rmse: 0.702887\tvalid_1's rmse: 0.840741\n",
            "[1925]\ttraining's rmse: 0.702194\tvalid_1's rmse: 0.840704\n",
            "[1950]\ttraining's rmse: 0.701539\tvalid_1's rmse: 0.840551\n",
            "[1975]\ttraining's rmse: 0.700731\tvalid_1's rmse: 0.840477\n",
            "[2000]\ttraining's rmse: 0.700035\tvalid_1's rmse: 0.840371\n",
            "[2025]\ttraining's rmse: 0.699436\tvalid_1's rmse: 0.840279\n",
            "[2050]\ttraining's rmse: 0.698651\tvalid_1's rmse: 0.840161\n",
            "[2075]\ttraining's rmse: 0.697917\tvalid_1's rmse: 0.839986\n",
            "[2100]\ttraining's rmse: 0.697087\tvalid_1's rmse: 0.839798\n",
            "[2125]\ttraining's rmse: 0.696209\tvalid_1's rmse: 0.839647\n",
            "[2150]\ttraining's rmse: 0.695587\tvalid_1's rmse: 0.839604\n",
            "[2175]\ttraining's rmse: 0.694992\tvalid_1's rmse: 0.839563\n",
            "[2200]\ttraining's rmse: 0.694307\tvalid_1's rmse: 0.839485\n",
            "[2225]\ttraining's rmse: 0.693586\tvalid_1's rmse: 0.839401\n",
            "[2250]\ttraining's rmse: 0.692958\tvalid_1's rmse: 0.839313\n",
            "[2275]\ttraining's rmse: 0.69233\tvalid_1's rmse: 0.839235\n",
            "[2300]\ttraining's rmse: 0.691768\tvalid_1's rmse: 0.83918\n",
            "[2325]\ttraining's rmse: 0.691074\tvalid_1's rmse: 0.83907\n",
            "[2350]\ttraining's rmse: 0.690472\tvalid_1's rmse: 0.839\n",
            "[2375]\ttraining's rmse: 0.689854\tvalid_1's rmse: 0.838943\n",
            "[2400]\ttraining's rmse: 0.689279\tvalid_1's rmse: 0.838903\n",
            "[2425]\ttraining's rmse: 0.688724\tvalid_1's rmse: 0.838882\n",
            "[2450]\ttraining's rmse: 0.688141\tvalid_1's rmse: 0.838872\n",
            "[2475]\ttraining's rmse: 0.687499\tvalid_1's rmse: 0.83877\n",
            "[2500]\ttraining's rmse: 0.686851\tvalid_1's rmse: 0.838698\n",
            "[2525]\ttraining's rmse: 0.686353\tvalid_1's rmse: 0.838672\n",
            "[2550]\ttraining's rmse: 0.685854\tvalid_1's rmse: 0.838606\n",
            "[2575]\ttraining's rmse: 0.685172\tvalid_1's rmse: 0.838501\n",
            "[2600]\ttraining's rmse: 0.684501\tvalid_1's rmse: 0.838373\n",
            "[2625]\ttraining's rmse: 0.683999\tvalid_1's rmse: 0.838324\n",
            "[2650]\ttraining's rmse: 0.683416\tvalid_1's rmse: 0.838278\n",
            "[2675]\ttraining's rmse: 0.682839\tvalid_1's rmse: 0.838192\n",
            "[2700]\ttraining's rmse: 0.68229\tvalid_1's rmse: 0.838108\n",
            "[2725]\ttraining's rmse: 0.681723\tvalid_1's rmse: 0.837994\n",
            "[2750]\ttraining's rmse: 0.68126\tvalid_1's rmse: 0.837949\n",
            "[2775]\ttraining's rmse: 0.680772\tvalid_1's rmse: 0.837877\n",
            "[2800]\ttraining's rmse: 0.680222\tvalid_1's rmse: 0.83777\n",
            "[2825]\ttraining's rmse: 0.679815\tvalid_1's rmse: 0.837761\n",
            "[2850]\ttraining's rmse: 0.679371\tvalid_1's rmse: 0.837745\n",
            "[2875]\ttraining's rmse: 0.678765\tvalid_1's rmse: 0.837685\n",
            "[2900]\ttraining's rmse: 0.67825\tvalid_1's rmse: 0.837639\n",
            "[2925]\ttraining's rmse: 0.677828\tvalid_1's rmse: 0.837609\n",
            "[2950]\ttraining's rmse: 0.677289\tvalid_1's rmse: 0.837531\n",
            "[2975]\ttraining's rmse: 0.67687\tvalid_1's rmse: 0.837494\n",
            "[3000]\ttraining's rmse: 0.676397\tvalid_1's rmse: 0.837455\n",
            "[3025]\ttraining's rmse: 0.675897\tvalid_1's rmse: 0.837377\n",
            "[3050]\ttraining's rmse: 0.675372\tvalid_1's rmse: 0.83731\n",
            "[3075]\ttraining's rmse: 0.674964\tvalid_1's rmse: 0.837242\n",
            "[3100]\ttraining's rmse: 0.674513\tvalid_1's rmse: 0.837218\n",
            "[3125]\ttraining's rmse: 0.673848\tvalid_1's rmse: 0.837231\n",
            "[3150]\ttraining's rmse: 0.673291\tvalid_1's rmse: 0.83722\n",
            "[3175]\ttraining's rmse: 0.672922\tvalid_1's rmse: 0.837205\n",
            "[3200]\ttraining's rmse: 0.672524\tvalid_1's rmse: 0.837151\n",
            "[3225]\ttraining's rmse: 0.671998\tvalid_1's rmse: 0.837025\n",
            "[3250]\ttraining's rmse: 0.671574\tvalid_1's rmse: 0.83699\n",
            "[3275]\ttraining's rmse: 0.671121\tvalid_1's rmse: 0.83693\n",
            "[3300]\ttraining's rmse: 0.670621\tvalid_1's rmse: 0.836945\n",
            "[3325]\ttraining's rmse: 0.67009\tvalid_1's rmse: 0.8369\n",
            "[3350]\ttraining's rmse: 0.669502\tvalid_1's rmse: 0.836844\n",
            "[3375]\ttraining's rmse: 0.669074\tvalid_1's rmse: 0.836824\n",
            "[3400]\ttraining's rmse: 0.66869\tvalid_1's rmse: 0.836736\n",
            "[3425]\ttraining's rmse: 0.668233\tvalid_1's rmse: 0.836673\n",
            "[3450]\ttraining's rmse: 0.667861\tvalid_1's rmse: 0.836651\n",
            "[3475]\ttraining's rmse: 0.667389\tvalid_1's rmse: 0.836664\n",
            "[3500]\ttraining's rmse: 0.666962\tvalid_1's rmse: 0.836659\n",
            "[3525]\ttraining's rmse: 0.666512\tvalid_1's rmse: 0.836618\n",
            "[3550]\ttraining's rmse: 0.666117\tvalid_1's rmse: 0.836601\n",
            "[3575]\ttraining's rmse: 0.665637\tvalid_1's rmse: 0.836501\n",
            "[3600]\ttraining's rmse: 0.665166\tvalid_1's rmse: 0.836484\n",
            "[3625]\ttraining's rmse: 0.664699\tvalid_1's rmse: 0.836401\n",
            "[3650]\ttraining's rmse: 0.664175\tvalid_1's rmse: 0.836341\n",
            "[3675]\ttraining's rmse: 0.663729\tvalid_1's rmse: 0.836271\n",
            "[3700]\ttraining's rmse: 0.663418\tvalid_1's rmse: 0.836257\n",
            "[3725]\ttraining's rmse: 0.66294\tvalid_1's rmse: 0.836204\n",
            "[3750]\ttraining's rmse: 0.662552\tvalid_1's rmse: 0.836137\n",
            "[3775]\ttraining's rmse: 0.66214\tvalid_1's rmse: 0.836119\n",
            "[3800]\ttraining's rmse: 0.661782\tvalid_1's rmse: 0.836139\n",
            "[3825]\ttraining's rmse: 0.661318\tvalid_1's rmse: 0.83612\n",
            "[3850]\ttraining's rmse: 0.660905\tvalid_1's rmse: 0.836065\n",
            "[3875]\ttraining's rmse: 0.660587\tvalid_1's rmse: 0.836062\n",
            "[3900]\ttraining's rmse: 0.660236\tvalid_1's rmse: 0.836057\n",
            "[3925]\ttraining's rmse: 0.659779\tvalid_1's rmse: 0.836061\n",
            "[3950]\ttraining's rmse: 0.659418\tvalid_1's rmse: 0.836002\n",
            "[3975]\ttraining's rmse: 0.658957\tvalid_1's rmse: 0.83599\n",
            "[4000]\ttraining's rmse: 0.658579\tvalid_1's rmse: 0.835967\n",
            "[4025]\ttraining's rmse: 0.658215\tvalid_1's rmse: 0.835946\n",
            "[4050]\ttraining's rmse: 0.657707\tvalid_1's rmse: 0.835908\n",
            "[4075]\ttraining's rmse: 0.657158\tvalid_1's rmse: 0.835893\n",
            "[4100]\ttraining's rmse: 0.656663\tvalid_1's rmse: 0.835849\n",
            "[4125]\ttraining's rmse: 0.656215\tvalid_1's rmse: 0.835794\n",
            "[4150]\ttraining's rmse: 0.655823\tvalid_1's rmse: 0.835775\n",
            "[4175]\ttraining's rmse: 0.655426\tvalid_1's rmse: 0.835775\n",
            "[4200]\ttraining's rmse: 0.655088\tvalid_1's rmse: 0.835759\n",
            "[4225]\ttraining's rmse: 0.654803\tvalid_1's rmse: 0.835745\n",
            "[4250]\ttraining's rmse: 0.654372\tvalid_1's rmse: 0.835732\n",
            "[4275]\ttraining's rmse: 0.653866\tvalid_1's rmse: 0.835662\n",
            "[4300]\ttraining's rmse: 0.653482\tvalid_1's rmse: 0.83567\n",
            "[4325]\ttraining's rmse: 0.653141\tvalid_1's rmse: 0.835622\n",
            "[4350]\ttraining's rmse: 0.652771\tvalid_1's rmse: 0.835609\n",
            "[4375]\ttraining's rmse: 0.652445\tvalid_1's rmse: 0.835589\n",
            "[4400]\ttraining's rmse: 0.651906\tvalid_1's rmse: 0.835492\n",
            "[4425]\ttraining's rmse: 0.65155\tvalid_1's rmse: 0.835494\n",
            "[4450]\ttraining's rmse: 0.651159\tvalid_1's rmse: 0.835462\n",
            "[4475]\ttraining's rmse: 0.650864\tvalid_1's rmse: 0.835459\n",
            "[4500]\ttraining's rmse: 0.650383\tvalid_1's rmse: 0.835434\n",
            "[4525]\ttraining's rmse: 0.650029\tvalid_1's rmse: 0.835429\n",
            "[4550]\ttraining's rmse: 0.649608\tvalid_1's rmse: 0.835438\n",
            "[4575]\ttraining's rmse: 0.649236\tvalid_1's rmse: 0.835434\n",
            "[4600]\ttraining's rmse: 0.648893\tvalid_1's rmse: 0.835414\n",
            "[4625]\ttraining's rmse: 0.648616\tvalid_1's rmse: 0.835398\n",
            "[4650]\ttraining's rmse: 0.64822\tvalid_1's rmse: 0.835414\n",
            "[4675]\ttraining's rmse: 0.647819\tvalid_1's rmse: 0.835359\n",
            "[4700]\ttraining's rmse: 0.647513\tvalid_1's rmse: 0.835347\n",
            "[4725]\ttraining's rmse: 0.647181\tvalid_1's rmse: 0.835338\n",
            "[4750]\ttraining's rmse: 0.646818\tvalid_1's rmse: 0.835297\n",
            "[4775]\ttraining's rmse: 0.646533\tvalid_1's rmse: 0.835272\n",
            "[4800]\ttraining's rmse: 0.646177\tvalid_1's rmse: 0.835281\n",
            "[4825]\ttraining's rmse: 0.645842\tvalid_1's rmse: 0.835291\n",
            "[4850]\ttraining's rmse: 0.645567\tvalid_1's rmse: 0.835288\n",
            "Early stopping, best iteration is:\n",
            "[4773]\ttraining's rmse: 0.646553\tvalid_1's rmse: 0.83527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
            "  warnings.warn('Using categorical_feature in Dataset.')\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
            "  warnings.warn('categorical_feature in param dict is overridden.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[25]\ttraining's rmse: 1.09631\tvalid_1's rmse: 1.25721\n",
            "[50]\ttraining's rmse: 1.02703\tvalid_1's rmse: 1.18384\n",
            "[75]\ttraining's rmse: 0.971912\tvalid_1's rmse: 1.12511\n",
            "[100]\ttraining's rmse: 0.929929\tvalid_1's rmse: 1.08048\n",
            "[125]\ttraining's rmse: 0.897742\tvalid_1's rmse: 1.04856\n",
            "[150]\ttraining's rmse: 0.872936\tvalid_1's rmse: 1.02387\n",
            "[175]\ttraining's rmse: 0.852977\tvalid_1's rmse: 1.00506\n",
            "[200]\ttraining's rmse: 0.837173\tvalid_1's rmse: 0.991012\n",
            "[225]\ttraining's rmse: 0.824258\tvalid_1's rmse: 0.980506\n",
            "[250]\ttraining's rmse: 0.813482\tvalid_1's rmse: 0.972381\n",
            "[275]\ttraining's rmse: 0.804516\tvalid_1's rmse: 0.96591\n",
            "[300]\ttraining's rmse: 0.796732\tvalid_1's rmse: 0.960898\n",
            "[325]\ttraining's rmse: 0.790252\tvalid_1's rmse: 0.957157\n",
            "[350]\ttraining's rmse: 0.784327\tvalid_1's rmse: 0.953729\n",
            "[375]\ttraining's rmse: 0.779299\tvalid_1's rmse: 0.951165\n",
            "[400]\ttraining's rmse: 0.774528\tvalid_1's rmse: 0.949049\n",
            "[425]\ttraining's rmse: 0.770141\tvalid_1's rmse: 0.947206\n",
            "[450]\ttraining's rmse: 0.765921\tvalid_1's rmse: 0.945652\n",
            "[475]\ttraining's rmse: 0.762353\tvalid_1's rmse: 0.944358\n",
            "[500]\ttraining's rmse: 0.758943\tvalid_1's rmse: 0.943131\n",
            "[525]\ttraining's rmse: 0.755757\tvalid_1's rmse: 0.942096\n",
            "[550]\ttraining's rmse: 0.752826\tvalid_1's rmse: 0.941223\n",
            "[575]\ttraining's rmse: 0.750314\tvalid_1's rmse: 0.940465\n",
            "[600]\ttraining's rmse: 0.747792\tvalid_1's rmse: 0.939759\n",
            "[625]\ttraining's rmse: 0.745513\tvalid_1's rmse: 0.939159\n",
            "[650]\ttraining's rmse: 0.74325\tvalid_1's rmse: 0.938615\n",
            "[675]\ttraining's rmse: 0.741103\tvalid_1's rmse: 0.938011\n",
            "[700]\ttraining's rmse: 0.738909\tvalid_1's rmse: 0.937675\n",
            "[725]\ttraining's rmse: 0.737036\tvalid_1's rmse: 0.937168\n",
            "[750]\ttraining's rmse: 0.735157\tvalid_1's rmse: 0.936752\n",
            "[775]\ttraining's rmse: 0.733426\tvalid_1's rmse: 0.936308\n",
            "[800]\ttraining's rmse: 0.731587\tvalid_1's rmse: 0.935968\n",
            "[825]\ttraining's rmse: 0.7299\tvalid_1's rmse: 0.9357\n",
            "[850]\ttraining's rmse: 0.728246\tvalid_1's rmse: 0.935225\n",
            "[875]\ttraining's rmse: 0.726809\tvalid_1's rmse: 0.93497\n",
            "[900]\ttraining's rmse: 0.725422\tvalid_1's rmse: 0.934819\n",
            "[925]\ttraining's rmse: 0.724074\tvalid_1's rmse: 0.934617\n",
            "[950]\ttraining's rmse: 0.722655\tvalid_1's rmse: 0.934316\n",
            "[975]\ttraining's rmse: 0.721411\tvalid_1's rmse: 0.934106\n",
            "[1000]\ttraining's rmse: 0.720197\tvalid_1's rmse: 0.933868\n",
            "[1025]\ttraining's rmse: 0.719037\tvalid_1's rmse: 0.933618\n",
            "[1050]\ttraining's rmse: 0.717816\tvalid_1's rmse: 0.933419\n",
            "[1075]\ttraining's rmse: 0.716537\tvalid_1's rmse: 0.933301\n",
            "[1100]\ttraining's rmse: 0.715507\tvalid_1's rmse: 0.933159\n",
            "[1125]\ttraining's rmse: 0.714318\tvalid_1's rmse: 0.93304\n",
            "[1150]\ttraining's rmse: 0.713201\tvalid_1's rmse: 0.932971\n",
            "[1175]\ttraining's rmse: 0.712139\tvalid_1's rmse: 0.932789\n",
            "[1200]\ttraining's rmse: 0.711103\tvalid_1's rmse: 0.932664\n",
            "[1225]\ttraining's rmse: 0.710123\tvalid_1's rmse: 0.932522\n",
            "[1250]\ttraining's rmse: 0.709028\tvalid_1's rmse: 0.932385\n",
            "[1275]\ttraining's rmse: 0.70802\tvalid_1's rmse: 0.93233\n",
            "[1300]\ttraining's rmse: 0.707183\tvalid_1's rmse: 0.932293\n",
            "[1325]\ttraining's rmse: 0.706099\tvalid_1's rmse: 0.932212\n",
            "[1350]\ttraining's rmse: 0.705165\tvalid_1's rmse: 0.932018\n",
            "[1375]\ttraining's rmse: 0.704252\tvalid_1's rmse: 0.931835\n",
            "[1400]\ttraining's rmse: 0.703337\tvalid_1's rmse: 0.931629\n",
            "[1425]\ttraining's rmse: 0.702528\tvalid_1's rmse: 0.931527\n",
            "[1450]\ttraining's rmse: 0.701539\tvalid_1's rmse: 0.931527\n",
            "[1475]\ttraining's rmse: 0.700719\tvalid_1's rmse: 0.931446\n",
            "[1500]\ttraining's rmse: 0.699889\tvalid_1's rmse: 0.931321\n",
            "[1525]\ttraining's rmse: 0.69915\tvalid_1's rmse: 0.931259\n",
            "[1550]\ttraining's rmse: 0.698372\tvalid_1's rmse: 0.931143\n",
            "[1575]\ttraining's rmse: 0.697532\tvalid_1's rmse: 0.931036\n",
            "[1600]\ttraining's rmse: 0.696739\tvalid_1's rmse: 0.930933\n",
            "[1625]\ttraining's rmse: 0.696017\tvalid_1's rmse: 0.930835\n",
            "[1650]\ttraining's rmse: 0.695311\tvalid_1's rmse: 0.93074\n",
            "[1675]\ttraining's rmse: 0.694463\tvalid_1's rmse: 0.930651\n",
            "[1700]\ttraining's rmse: 0.693719\tvalid_1's rmse: 0.93059\n",
            "[1725]\ttraining's rmse: 0.692914\tvalid_1's rmse: 0.930506\n",
            "[1750]\ttraining's rmse: 0.692293\tvalid_1's rmse: 0.930456\n",
            "[1775]\ttraining's rmse: 0.691387\tvalid_1's rmse: 0.930357\n",
            "[1800]\ttraining's rmse: 0.690575\tvalid_1's rmse: 0.930295\n",
            "[1825]\ttraining's rmse: 0.68979\tvalid_1's rmse: 0.930211\n",
            "[1850]\ttraining's rmse: 0.689102\tvalid_1's rmse: 0.930158\n",
            "[1875]\ttraining's rmse: 0.688422\tvalid_1's rmse: 0.93006\n",
            "[1900]\ttraining's rmse: 0.687767\tvalid_1's rmse: 0.929974\n",
            "[1925]\ttraining's rmse: 0.687013\tvalid_1's rmse: 0.929891\n",
            "[1950]\ttraining's rmse: 0.686217\tvalid_1's rmse: 0.929834\n",
            "[1975]\ttraining's rmse: 0.685496\tvalid_1's rmse: 0.929744\n",
            "[2000]\ttraining's rmse: 0.684824\tvalid_1's rmse: 0.929616\n",
            "[2025]\ttraining's rmse: 0.684265\tvalid_1's rmse: 0.929546\n",
            "[2050]\ttraining's rmse: 0.683289\tvalid_1's rmse: 0.929498\n",
            "[2075]\ttraining's rmse: 0.682543\tvalid_1's rmse: 0.929477\n",
            "[2100]\ttraining's rmse: 0.681875\tvalid_1's rmse: 0.929415\n",
            "[2125]\ttraining's rmse: 0.681077\tvalid_1's rmse: 0.929337\n",
            "[2150]\ttraining's rmse: 0.680527\tvalid_1's rmse: 0.929272\n",
            "[2175]\ttraining's rmse: 0.679944\tvalid_1's rmse: 0.929198\n",
            "[2200]\ttraining's rmse: 0.6792\tvalid_1's rmse: 0.929215\n",
            "[2225]\ttraining's rmse: 0.678654\tvalid_1's rmse: 0.929136\n",
            "[2250]\ttraining's rmse: 0.678001\tvalid_1's rmse: 0.929069\n",
            "[2275]\ttraining's rmse: 0.677347\tvalid_1's rmse: 0.928974\n",
            "[2300]\ttraining's rmse: 0.676771\tvalid_1's rmse: 0.92892\n",
            "[2325]\ttraining's rmse: 0.676209\tvalid_1's rmse: 0.928879\n",
            "[2350]\ttraining's rmse: 0.675527\tvalid_1's rmse: 0.92883\n",
            "[2375]\ttraining's rmse: 0.674936\tvalid_1's rmse: 0.928788\n",
            "[2400]\ttraining's rmse: 0.674398\tvalid_1's rmse: 0.928726\n",
            "[2425]\ttraining's rmse: 0.673864\tvalid_1's rmse: 0.928682\n",
            "[2450]\ttraining's rmse: 0.673353\tvalid_1's rmse: 0.928627\n",
            "[2475]\ttraining's rmse: 0.672739\tvalid_1's rmse: 0.928607\n",
            "[2500]\ttraining's rmse: 0.672173\tvalid_1's rmse: 0.928585\n",
            "[2525]\ttraining's rmse: 0.671673\tvalid_1's rmse: 0.928549\n",
            "[2550]\ttraining's rmse: 0.671134\tvalid_1's rmse: 0.928492\n",
            "[2575]\ttraining's rmse: 0.670594\tvalid_1's rmse: 0.92846\n",
            "[2600]\ttraining's rmse: 0.670127\tvalid_1's rmse: 0.928434\n",
            "[2625]\ttraining's rmse: 0.669626\tvalid_1's rmse: 0.928406\n",
            "[2650]\ttraining's rmse: 0.669127\tvalid_1's rmse: 0.928356\n",
            "[2675]\ttraining's rmse: 0.668604\tvalid_1's rmse: 0.928341\n",
            "[2700]\ttraining's rmse: 0.668008\tvalid_1's rmse: 0.928279\n",
            "[2725]\ttraining's rmse: 0.66753\tvalid_1's rmse: 0.928247\n",
            "[2750]\ttraining's rmse: 0.667039\tvalid_1's rmse: 0.928225\n",
            "[2775]\ttraining's rmse: 0.666582\tvalid_1's rmse: 0.928192\n",
            "[2800]\ttraining's rmse: 0.666164\tvalid_1's rmse: 0.928131\n",
            "[2825]\ttraining's rmse: 0.665723\tvalid_1's rmse: 0.928073\n",
            "[2850]\ttraining's rmse: 0.665265\tvalid_1's rmse: 0.928031\n",
            "[2875]\ttraining's rmse: 0.664757\tvalid_1's rmse: 0.927975\n",
            "[2900]\ttraining's rmse: 0.664364\tvalid_1's rmse: 0.927949\n",
            "[2925]\ttraining's rmse: 0.663957\tvalid_1's rmse: 0.927928\n",
            "[2950]\ttraining's rmse: 0.663571\tvalid_1's rmse: 0.927905\n",
            "[2975]\ttraining's rmse: 0.662929\tvalid_1's rmse: 0.927879\n",
            "[3000]\ttraining's rmse: 0.662514\tvalid_1's rmse: 0.927829\n",
            "[3025]\ttraining's rmse: 0.661988\tvalid_1's rmse: 0.927838\n",
            "[3050]\ttraining's rmse: 0.661543\tvalid_1's rmse: 0.927795\n",
            "[3075]\ttraining's rmse: 0.661126\tvalid_1's rmse: 0.927753\n",
            "[3100]\ttraining's rmse: 0.660642\tvalid_1's rmse: 0.927694\n",
            "[3125]\ttraining's rmse: 0.660058\tvalid_1's rmse: 0.927621\n",
            "[3150]\ttraining's rmse: 0.659527\tvalid_1's rmse: 0.927587\n",
            "[3175]\ttraining's rmse: 0.659174\tvalid_1's rmse: 0.927601\n",
            "[3200]\ttraining's rmse: 0.658787\tvalid_1's rmse: 0.92757\n",
            "[3225]\ttraining's rmse: 0.658237\tvalid_1's rmse: 0.927548\n",
            "[3250]\ttraining's rmse: 0.657791\tvalid_1's rmse: 0.927509\n",
            "[3275]\ttraining's rmse: 0.657417\tvalid_1's rmse: 0.927454\n",
            "[3300]\ttraining's rmse: 0.657052\tvalid_1's rmse: 0.927445\n",
            "[3325]\ttraining's rmse: 0.65634\tvalid_1's rmse: 0.927498\n",
            "[3350]\ttraining's rmse: 0.655792\tvalid_1's rmse: 0.927473\n",
            "[3375]\ttraining's rmse: 0.655408\tvalid_1's rmse: 0.927467\n",
            "Early stopping, best iteration is:\n",
            "[3298]\ttraining's rmse: 0.65708\tvalid_1's rmse: 0.927436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
            "  warnings.warn('Using categorical_feature in Dataset.')\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
            "  warnings.warn('categorical_feature in param dict is overridden.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[25]\ttraining's rmse: 1.14491\tvalid_1's rmse: 1.06504\n",
            "[50]\ttraining's rmse: 1.07227\tvalid_1's rmse: 1.00172\n",
            "[75]\ttraining's rmse: 1.01449\tvalid_1's rmse: 0.952549\n",
            "[100]\ttraining's rmse: 0.97015\tvalid_1's rmse: 0.91592\n",
            "[125]\ttraining's rmse: 0.936286\tvalid_1's rmse: 0.889148\n",
            "[150]\ttraining's rmse: 0.910151\tvalid_1's rmse: 0.869374\n",
            "[175]\ttraining's rmse: 0.889045\tvalid_1's rmse: 0.854515\n",
            "[200]\ttraining's rmse: 0.872364\tvalid_1's rmse: 0.843324\n",
            "[225]\ttraining's rmse: 0.858772\tvalid_1's rmse: 0.835225\n",
            "[250]\ttraining's rmse: 0.847595\tvalid_1's rmse: 0.828328\n",
            "[275]\ttraining's rmse: 0.838207\tvalid_1's rmse: 0.822998\n",
            "[300]\ttraining's rmse: 0.830053\tvalid_1's rmse: 0.81911\n",
            "[325]\ttraining's rmse: 0.823325\tvalid_1's rmse: 0.815826\n",
            "[350]\ttraining's rmse: 0.817132\tvalid_1's rmse: 0.812912\n",
            "[375]\ttraining's rmse: 0.811774\tvalid_1's rmse: 0.810844\n",
            "[400]\ttraining's rmse: 0.806842\tvalid_1's rmse: 0.809106\n",
            "[425]\ttraining's rmse: 0.802138\tvalid_1's rmse: 0.807462\n",
            "[450]\ttraining's rmse: 0.797675\tvalid_1's rmse: 0.80597\n",
            "[475]\ttraining's rmse: 0.793859\tvalid_1's rmse: 0.804954\n",
            "[500]\ttraining's rmse: 0.79026\tvalid_1's rmse: 0.803936\n",
            "[525]\ttraining's rmse: 0.786952\tvalid_1's rmse: 0.803055\n",
            "[550]\ttraining's rmse: 0.783876\tvalid_1's rmse: 0.802318\n",
            "[575]\ttraining's rmse: 0.781191\tvalid_1's rmse: 0.801716\n",
            "[600]\ttraining's rmse: 0.778517\tvalid_1's rmse: 0.801161\n",
            "[625]\ttraining's rmse: 0.775983\tvalid_1's rmse: 0.800631\n",
            "[650]\ttraining's rmse: 0.77349\tvalid_1's rmse: 0.800125\n",
            "[675]\ttraining's rmse: 0.771095\tvalid_1's rmse: 0.799492\n",
            "[700]\ttraining's rmse: 0.768806\tvalid_1's rmse: 0.798993\n",
            "[725]\ttraining's rmse: 0.766815\tvalid_1's rmse: 0.798593\n",
            "[750]\ttraining's rmse: 0.764898\tvalid_1's rmse: 0.798155\n",
            "[775]\ttraining's rmse: 0.763028\tvalid_1's rmse: 0.797826\n",
            "[800]\ttraining's rmse: 0.761028\tvalid_1's rmse: 0.79745\n",
            "[825]\ttraining's rmse: 0.759288\tvalid_1's rmse: 0.797255\n",
            "[850]\ttraining's rmse: 0.757553\tvalid_1's rmse: 0.796791\n",
            "[875]\ttraining's rmse: 0.755884\tvalid_1's rmse: 0.796552\n",
            "[900]\ttraining's rmse: 0.75437\tvalid_1's rmse: 0.796295\n",
            "[925]\ttraining's rmse: 0.752862\tvalid_1's rmse: 0.796057\n",
            "[950]\ttraining's rmse: 0.751238\tvalid_1's rmse: 0.795719\n",
            "[975]\ttraining's rmse: 0.749867\tvalid_1's rmse: 0.795616\n",
            "[1000]\ttraining's rmse: 0.748553\tvalid_1's rmse: 0.79546\n",
            "[1025]\ttraining's rmse: 0.747337\tvalid_1's rmse: 0.795301\n",
            "[1050]\ttraining's rmse: 0.746027\tvalid_1's rmse: 0.795142\n",
            "[1075]\ttraining's rmse: 0.744683\tvalid_1's rmse: 0.79494\n",
            "[1100]\ttraining's rmse: 0.743508\tvalid_1's rmse: 0.794796\n",
            "[1125]\ttraining's rmse: 0.742216\tvalid_1's rmse: 0.794658\n",
            "[1150]\ttraining's rmse: 0.741016\tvalid_1's rmse: 0.794528\n",
            "[1175]\ttraining's rmse: 0.739849\tvalid_1's rmse: 0.794418\n",
            "[1200]\ttraining's rmse: 0.738719\tvalid_1's rmse: 0.794241\n",
            "[1225]\ttraining's rmse: 0.737684\tvalid_1's rmse: 0.794099\n",
            "[1250]\ttraining's rmse: 0.736518\tvalid_1's rmse: 0.793923\n",
            "[1275]\ttraining's rmse: 0.735424\tvalid_1's rmse: 0.79381\n",
            "[1300]\ttraining's rmse: 0.734544\tvalid_1's rmse: 0.793743\n",
            "[1325]\ttraining's rmse: 0.733462\tvalid_1's rmse: 0.793578\n",
            "[1350]\ttraining's rmse: 0.7325\tvalid_1's rmse: 0.793422\n",
            "[1375]\ttraining's rmse: 0.731472\tvalid_1's rmse: 0.793259\n",
            "[1400]\ttraining's rmse: 0.730548\tvalid_1's rmse: 0.793139\n",
            "[1425]\ttraining's rmse: 0.729681\tvalid_1's rmse: 0.793083\n",
            "[1450]\ttraining's rmse: 0.728733\tvalid_1's rmse: 0.792949\n",
            "[1475]\ttraining's rmse: 0.727784\tvalid_1's rmse: 0.79283\n",
            "[1500]\ttraining's rmse: 0.726814\tvalid_1's rmse: 0.792657\n",
            "[1525]\ttraining's rmse: 0.726025\tvalid_1's rmse: 0.792601\n",
            "[1550]\ttraining's rmse: 0.725139\tvalid_1's rmse: 0.79246\n",
            "[1575]\ttraining's rmse: 0.724104\tvalid_1's rmse: 0.792305\n",
            "[1600]\ttraining's rmse: 0.72326\tvalid_1's rmse: 0.792218\n",
            "[1625]\ttraining's rmse: 0.722453\tvalid_1's rmse: 0.792133\n",
            "[1650]\ttraining's rmse: 0.721734\tvalid_1's rmse: 0.79202\n",
            "[1675]\ttraining's rmse: 0.720944\tvalid_1's rmse: 0.791909\n",
            "[1700]\ttraining's rmse: 0.720119\tvalid_1's rmse: 0.791821\n",
            "[1725]\ttraining's rmse: 0.719312\tvalid_1's rmse: 0.791766\n",
            "[1750]\ttraining's rmse: 0.718639\tvalid_1's rmse: 0.791723\n",
            "[1775]\ttraining's rmse: 0.717711\tvalid_1's rmse: 0.791657\n",
            "[1800]\ttraining's rmse: 0.716706\tvalid_1's rmse: 0.791542\n",
            "[1825]\ttraining's rmse: 0.715866\tvalid_1's rmse: 0.791442\n",
            "[1850]\ttraining's rmse: 0.715178\tvalid_1's rmse: 0.791378\n",
            "[1875]\ttraining's rmse: 0.71454\tvalid_1's rmse: 0.791331\n",
            "[1900]\ttraining's rmse: 0.713834\tvalid_1's rmse: 0.791271\n",
            "[1925]\ttraining's rmse: 0.71303\tvalid_1's rmse: 0.791193\n",
            "[1950]\ttraining's rmse: 0.712283\tvalid_1's rmse: 0.791063\n",
            "[1975]\ttraining's rmse: 0.71155\tvalid_1's rmse: 0.790998\n",
            "[2000]\ttraining's rmse: 0.710789\tvalid_1's rmse: 0.790902\n",
            "[2025]\ttraining's rmse: 0.710188\tvalid_1's rmse: 0.790839\n",
            "[2050]\ttraining's rmse: 0.709188\tvalid_1's rmse: 0.790662\n",
            "[2075]\ttraining's rmse: 0.708526\tvalid_1's rmse: 0.790594\n",
            "[2100]\ttraining's rmse: 0.707864\tvalid_1's rmse: 0.790545\n",
            "[2125]\ttraining's rmse: 0.707156\tvalid_1's rmse: 0.790467\n",
            "[2150]\ttraining's rmse: 0.706407\tvalid_1's rmse: 0.790425\n",
            "[2175]\ttraining's rmse: 0.705779\tvalid_1's rmse: 0.790373\n",
            "[2200]\ttraining's rmse: 0.705038\tvalid_1's rmse: 0.790278\n",
            "[2225]\ttraining's rmse: 0.704418\tvalid_1's rmse: 0.790235\n",
            "[2250]\ttraining's rmse: 0.703772\tvalid_1's rmse: 0.79018\n",
            "[2275]\ttraining's rmse: 0.703207\tvalid_1's rmse: 0.79014\n",
            "[2300]\ttraining's rmse: 0.702631\tvalid_1's rmse: 0.79011\n",
            "[2325]\ttraining's rmse: 0.702011\tvalid_1's rmse: 0.790053\n",
            "[2350]\ttraining's rmse: 0.701432\tvalid_1's rmse: 0.789984\n",
            "[2375]\ttraining's rmse: 0.700889\tvalid_1's rmse: 0.78994\n",
            "[2400]\ttraining's rmse: 0.700298\tvalid_1's rmse: 0.789905\n",
            "[2425]\ttraining's rmse: 0.699759\tvalid_1's rmse: 0.789831\n",
            "[2450]\ttraining's rmse: 0.699196\tvalid_1's rmse: 0.789825\n",
            "[2475]\ttraining's rmse: 0.698586\tvalid_1's rmse: 0.789773\n",
            "[2500]\ttraining's rmse: 0.698045\tvalid_1's rmse: 0.789736\n",
            "[2525]\ttraining's rmse: 0.697502\tvalid_1's rmse: 0.789689\n",
            "[2550]\ttraining's rmse: 0.696995\tvalid_1's rmse: 0.789675\n",
            "[2575]\ttraining's rmse: 0.696369\tvalid_1's rmse: 0.789629\n",
            "[2600]\ttraining's rmse: 0.695882\tvalid_1's rmse: 0.789605\n",
            "[2625]\ttraining's rmse: 0.695339\tvalid_1's rmse: 0.789568\n",
            "[2650]\ttraining's rmse: 0.694816\tvalid_1's rmse: 0.789519\n",
            "[2675]\ttraining's rmse: 0.694083\tvalid_1's rmse: 0.789455\n",
            "[2700]\ttraining's rmse: 0.693411\tvalid_1's rmse: 0.789389\n",
            "[2725]\ttraining's rmse: 0.692907\tvalid_1's rmse: 0.789334\n",
            "[2750]\ttraining's rmse: 0.692402\tvalid_1's rmse: 0.789296\n",
            "[2775]\ttraining's rmse: 0.691932\tvalid_1's rmse: 0.789254\n",
            "[2800]\ttraining's rmse: 0.691256\tvalid_1's rmse: 0.789228\n",
            "[2825]\ttraining's rmse: 0.690786\tvalid_1's rmse: 0.789206\n",
            "[2850]\ttraining's rmse: 0.690298\tvalid_1's rmse: 0.789163\n",
            "[2875]\ttraining's rmse: 0.689824\tvalid_1's rmse: 0.789105\n",
            "[2900]\ttraining's rmse: 0.689361\tvalid_1's rmse: 0.78906\n",
            "[2925]\ttraining's rmse: 0.688894\tvalid_1's rmse: 0.788991\n",
            "[2950]\ttraining's rmse: 0.688446\tvalid_1's rmse: 0.788926\n",
            "[2975]\ttraining's rmse: 0.687848\tvalid_1's rmse: 0.788877\n",
            "[3000]\ttraining's rmse: 0.687358\tvalid_1's rmse: 0.788824\n",
            "[3025]\ttraining's rmse: 0.686783\tvalid_1's rmse: 0.788753\n",
            "[3050]\ttraining's rmse: 0.686276\tvalid_1's rmse: 0.788739\n",
            "[3075]\ttraining's rmse: 0.685833\tvalid_1's rmse: 0.788704\n",
            "[3100]\ttraining's rmse: 0.685343\tvalid_1's rmse: 0.788673\n",
            "[3125]\ttraining's rmse: 0.684833\tvalid_1's rmse: 0.78862\n",
            "[3150]\ttraining's rmse: 0.684432\tvalid_1's rmse: 0.788596\n",
            "[3175]\ttraining's rmse: 0.684039\tvalid_1's rmse: 0.788581\n",
            "[3200]\ttraining's rmse: 0.683577\tvalid_1's rmse: 0.788528\n",
            "[3225]\ttraining's rmse: 0.683086\tvalid_1's rmse: 0.78848\n",
            "[3250]\ttraining's rmse: 0.682711\tvalid_1's rmse: 0.788459\n",
            "[3275]\ttraining's rmse: 0.682274\tvalid_1's rmse: 0.788447\n",
            "[3300]\ttraining's rmse: 0.681864\tvalid_1's rmse: 0.788427\n",
            "[3325]\ttraining's rmse: 0.681364\tvalid_1's rmse: 0.788389\n",
            "[3350]\ttraining's rmse: 0.680775\tvalid_1's rmse: 0.788354\n",
            "[3375]\ttraining's rmse: 0.68035\tvalid_1's rmse: 0.788317\n",
            "[3400]\ttraining's rmse: 0.679948\tvalid_1's rmse: 0.7883\n",
            "[3425]\ttraining's rmse: 0.679318\tvalid_1's rmse: 0.788219\n",
            "[3450]\ttraining's rmse: 0.678831\tvalid_1's rmse: 0.788177\n",
            "[3475]\ttraining's rmse: 0.678427\tvalid_1's rmse: 0.788162\n",
            "[3500]\ttraining's rmse: 0.678031\tvalid_1's rmse: 0.788136\n",
            "[3525]\ttraining's rmse: 0.677655\tvalid_1's rmse: 0.788099\n",
            "[3550]\ttraining's rmse: 0.677215\tvalid_1's rmse: 0.788091\n",
            "[3575]\ttraining's rmse: 0.676679\tvalid_1's rmse: 0.788037\n",
            "[3600]\ttraining's rmse: 0.676139\tvalid_1's rmse: 0.788\n",
            "[3625]\ttraining's rmse: 0.675709\tvalid_1's rmse: 0.787989\n",
            "[3650]\ttraining's rmse: 0.675319\tvalid_1's rmse: 0.787963\n",
            "[3675]\ttraining's rmse: 0.674906\tvalid_1's rmse: 0.787924\n",
            "[3700]\ttraining's rmse: 0.674558\tvalid_1's rmse: 0.787911\n",
            "[3725]\ttraining's rmse: 0.674107\tvalid_1's rmse: 0.78788\n",
            "[3750]\ttraining's rmse: 0.673744\tvalid_1's rmse: 0.78786\n",
            "[3775]\ttraining's rmse: 0.67328\tvalid_1's rmse: 0.787844\n",
            "[3800]\ttraining's rmse: 0.672718\tvalid_1's rmse: 0.787837\n",
            "[3825]\ttraining's rmse: 0.672336\tvalid_1's rmse: 0.787815\n",
            "[3850]\ttraining's rmse: 0.671982\tvalid_1's rmse: 0.787803\n",
            "[3875]\ttraining's rmse: 0.671573\tvalid_1's rmse: 0.787775\n",
            "[3900]\ttraining's rmse: 0.671284\tvalid_1's rmse: 0.787752\n",
            "[3925]\ttraining's rmse: 0.670887\tvalid_1's rmse: 0.787709\n",
            "[3950]\ttraining's rmse: 0.670357\tvalid_1's rmse: 0.78769\n",
            "[3975]\ttraining's rmse: 0.669934\tvalid_1's rmse: 0.787654\n",
            "[4000]\ttraining's rmse: 0.66953\tvalid_1's rmse: 0.78764\n",
            "[4025]\ttraining's rmse: 0.669155\tvalid_1's rmse: 0.787604\n",
            "[4050]\ttraining's rmse: 0.668693\tvalid_1's rmse: 0.787554\n",
            "[4075]\ttraining's rmse: 0.668301\tvalid_1's rmse: 0.787564\n",
            "[4100]\ttraining's rmse: 0.667908\tvalid_1's rmse: 0.787555\n",
            "[4125]\ttraining's rmse: 0.667553\tvalid_1's rmse: 0.787528\n",
            "[4150]\ttraining's rmse: 0.667166\tvalid_1's rmse: 0.787502\n",
            "[4175]\ttraining's rmse: 0.666761\tvalid_1's rmse: 0.787479\n",
            "[4200]\ttraining's rmse: 0.666426\tvalid_1's rmse: 0.787466\n",
            "[4225]\ttraining's rmse: 0.666109\tvalid_1's rmse: 0.787439\n",
            "[4250]\ttraining's rmse: 0.665735\tvalid_1's rmse: 0.787442\n",
            "[4275]\ttraining's rmse: 0.665249\tvalid_1's rmse: 0.787411\n",
            "[4300]\ttraining's rmse: 0.664943\tvalid_1's rmse: 0.787395\n",
            "[4325]\ttraining's rmse: 0.664623\tvalid_1's rmse: 0.787406\n",
            "[4350]\ttraining's rmse: 0.664267\tvalid_1's rmse: 0.787355\n",
            "[4375]\ttraining's rmse: 0.663793\tvalid_1's rmse: 0.787326\n",
            "[4400]\ttraining's rmse: 0.66341\tvalid_1's rmse: 0.787292\n",
            "[4425]\ttraining's rmse: 0.663126\tvalid_1's rmse: 0.787271\n",
            "[4450]\ttraining's rmse: 0.662723\tvalid_1's rmse: 0.787294\n",
            "[4475]\ttraining's rmse: 0.66237\tvalid_1's rmse: 0.787284\n",
            "[4500]\ttraining's rmse: 0.662069\tvalid_1's rmse: 0.787248\n",
            "[4525]\ttraining's rmse: 0.661761\tvalid_1's rmse: 0.787217\n",
            "[4550]\ttraining's rmse: 0.661488\tvalid_1's rmse: 0.787193\n",
            "[4575]\ttraining's rmse: 0.661172\tvalid_1's rmse: 0.787159\n",
            "[4600]\ttraining's rmse: 0.660801\tvalid_1's rmse: 0.787129\n",
            "[4625]\ttraining's rmse: 0.660463\tvalid_1's rmse: 0.787116\n",
            "[4650]\ttraining's rmse: 0.659986\tvalid_1's rmse: 0.787084\n",
            "[4675]\ttraining's rmse: 0.65961\tvalid_1's rmse: 0.787082\n",
            "[4700]\ttraining's rmse: 0.659244\tvalid_1's rmse: 0.787047\n",
            "[4725]\ttraining's rmse: 0.658943\tvalid_1's rmse: 0.787032\n",
            "[4750]\ttraining's rmse: 0.658596\tvalid_1's rmse: 0.78701\n",
            "[4775]\ttraining's rmse: 0.6583\tvalid_1's rmse: 0.786974\n",
            "[4800]\ttraining's rmse: 0.657982\tvalid_1's rmse: 0.786971\n",
            "[4825]\ttraining's rmse: 0.657736\tvalid_1's rmse: 0.786956\n",
            "[4850]\ttraining's rmse: 0.657405\tvalid_1's rmse: 0.786946\n",
            "[4875]\ttraining's rmse: 0.657102\tvalid_1's rmse: 0.786936\n",
            "[4900]\ttraining's rmse: 0.656755\tvalid_1's rmse: 0.78691\n",
            "[4925]\ttraining's rmse: 0.656393\tvalid_1's rmse: 0.786902\n",
            "[4950]\ttraining's rmse: 0.656041\tvalid_1's rmse: 0.786893\n",
            "[4975]\ttraining's rmse: 0.655624\tvalid_1's rmse: 0.786886\n",
            "[5000]\ttraining's rmse: 0.65523\tvalid_1's rmse: 0.786876\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[5000]\ttraining's rmse: 0.65523\tvalid_1's rmse: 0.786876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
            "  warnings.warn('Using categorical_feature in Dataset.')\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
            "  warnings.warn('categorical_feature in param dict is overridden.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[25]\ttraining's rmse: 1.10123\tvalid_1's rmse: 1.24023\n",
            "[50]\ttraining's rmse: 1.03177\tvalid_1's rmse: 1.16858\n",
            "[75]\ttraining's rmse: 0.976831\tvalid_1's rmse: 1.11051\n",
            "[100]\ttraining's rmse: 0.934683\tvalid_1's rmse: 1.06581\n",
            "[125]\ttraining's rmse: 0.902476\tvalid_1's rmse: 1.03365\n",
            "[150]\ttraining's rmse: 0.877881\tvalid_1's rmse: 1.00853\n",
            "[175]\ttraining's rmse: 0.858002\tvalid_1's rmse: 0.98829\n",
            "[200]\ttraining's rmse: 0.842294\tvalid_1's rmse: 0.973626\n",
            "[225]\ttraining's rmse: 0.829399\tvalid_1's rmse: 0.962402\n",
            "[250]\ttraining's rmse: 0.818665\tvalid_1's rmse: 0.953218\n",
            "[275]\ttraining's rmse: 0.809777\tvalid_1's rmse: 0.946477\n",
            "[300]\ttraining's rmse: 0.802062\tvalid_1's rmse: 0.941032\n",
            "[325]\ttraining's rmse: 0.795613\tvalid_1's rmse: 0.936483\n",
            "[350]\ttraining's rmse: 0.789695\tvalid_1's rmse: 0.932646\n",
            "[375]\ttraining's rmse: 0.784565\tvalid_1's rmse: 0.929733\n",
            "[400]\ttraining's rmse: 0.779803\tvalid_1's rmse: 0.927382\n",
            "[425]\ttraining's rmse: 0.775326\tvalid_1's rmse: 0.92513\n",
            "[450]\ttraining's rmse: 0.770924\tvalid_1's rmse: 0.923323\n",
            "[475]\ttraining's rmse: 0.767322\tvalid_1's rmse: 0.921767\n",
            "[500]\ttraining's rmse: 0.763899\tvalid_1's rmse: 0.920236\n",
            "[525]\ttraining's rmse: 0.760698\tvalid_1's rmse: 0.918802\n",
            "[550]\ttraining's rmse: 0.75774\tvalid_1's rmse: 0.91755\n",
            "[575]\ttraining's rmse: 0.755076\tvalid_1's rmse: 0.916597\n",
            "[600]\ttraining's rmse: 0.752436\tvalid_1's rmse: 0.915881\n",
            "[625]\ttraining's rmse: 0.750018\tvalid_1's rmse: 0.914998\n",
            "[650]\ttraining's rmse: 0.747649\tvalid_1's rmse: 0.914332\n",
            "[675]\ttraining's rmse: 0.745475\tvalid_1's rmse: 0.913577\n",
            "[700]\ttraining's rmse: 0.743154\tvalid_1's rmse: 0.912768\n",
            "[725]\ttraining's rmse: 0.741141\tvalid_1's rmse: 0.912026\n",
            "[750]\ttraining's rmse: 0.739206\tvalid_1's rmse: 0.911373\n",
            "[775]\ttraining's rmse: 0.737392\tvalid_1's rmse: 0.910781\n",
            "[800]\ttraining's rmse: 0.735379\tvalid_1's rmse: 0.910117\n",
            "[825]\ttraining's rmse: 0.733768\tvalid_1's rmse: 0.90968\n",
            "[850]\ttraining's rmse: 0.732159\tvalid_1's rmse: 0.909086\n",
            "[875]\ttraining's rmse: 0.730706\tvalid_1's rmse: 0.908773\n",
            "[900]\ttraining's rmse: 0.729381\tvalid_1's rmse: 0.908506\n",
            "[925]\ttraining's rmse: 0.727848\tvalid_1's rmse: 0.908141\n",
            "[950]\ttraining's rmse: 0.72642\tvalid_1's rmse: 0.907793\n",
            "[975]\ttraining's rmse: 0.725178\tvalid_1's rmse: 0.907553\n",
            "[1000]\ttraining's rmse: 0.723942\tvalid_1's rmse: 0.907334\n",
            "[1025]\ttraining's rmse: 0.722757\tvalid_1's rmse: 0.907091\n",
            "[1050]\ttraining's rmse: 0.721496\tvalid_1's rmse: 0.906812\n",
            "[1075]\ttraining's rmse: 0.72017\tvalid_1's rmse: 0.906514\n",
            "[1100]\ttraining's rmse: 0.718981\tvalid_1's rmse: 0.906362\n",
            "[1125]\ttraining's rmse: 0.717857\tvalid_1's rmse: 0.906203\n",
            "[1150]\ttraining's rmse: 0.716708\tvalid_1's rmse: 0.90607\n",
            "[1175]\ttraining's rmse: 0.715557\tvalid_1's rmse: 0.905874\n",
            "[1200]\ttraining's rmse: 0.714532\tvalid_1's rmse: 0.905706\n",
            "[1225]\ttraining's rmse: 0.713508\tvalid_1's rmse: 0.905521\n",
            "[1250]\ttraining's rmse: 0.71231\tvalid_1's rmse: 0.905332\n",
            "[1275]\ttraining's rmse: 0.711246\tvalid_1's rmse: 0.905129\n",
            "[1300]\ttraining's rmse: 0.710344\tvalid_1's rmse: 0.904998\n",
            "[1325]\ttraining's rmse: 0.709387\tvalid_1's rmse: 0.904834\n",
            "[1350]\ttraining's rmse: 0.708339\tvalid_1's rmse: 0.90462\n",
            "[1375]\ttraining's rmse: 0.707318\tvalid_1's rmse: 0.904408\n",
            "[1400]\ttraining's rmse: 0.70634\tvalid_1's rmse: 0.904165\n",
            "[1425]\ttraining's rmse: 0.705441\tvalid_1's rmse: 0.904065\n",
            "[1450]\ttraining's rmse: 0.704481\tvalid_1's rmse: 0.903957\n",
            "[1475]\ttraining's rmse: 0.70365\tvalid_1's rmse: 0.903816\n",
            "[1500]\ttraining's rmse: 0.702637\tvalid_1's rmse: 0.903636\n",
            "[1525]\ttraining's rmse: 0.701911\tvalid_1's rmse: 0.903511\n",
            "[1550]\ttraining's rmse: 0.70104\tvalid_1's rmse: 0.903344\n",
            "[1575]\ttraining's rmse: 0.699975\tvalid_1's rmse: 0.903149\n",
            "[1600]\ttraining's rmse: 0.699146\tvalid_1's rmse: 0.903076\n",
            "[1625]\ttraining's rmse: 0.698397\tvalid_1's rmse: 0.902951\n",
            "[1650]\ttraining's rmse: 0.697684\tvalid_1's rmse: 0.902866\n",
            "[1675]\ttraining's rmse: 0.696841\tvalid_1's rmse: 0.902723\n",
            "[1700]\ttraining's rmse: 0.696101\tvalid_1's rmse: 0.902637\n",
            "[1725]\ttraining's rmse: 0.695326\tvalid_1's rmse: 0.902488\n",
            "[1750]\ttraining's rmse: 0.694656\tvalid_1's rmse: 0.902385\n",
            "[1775]\ttraining's rmse: 0.693733\tvalid_1's rmse: 0.902244\n",
            "[1800]\ttraining's rmse: 0.692995\tvalid_1's rmse: 0.902151\n",
            "[1825]\ttraining's rmse: 0.692329\tvalid_1's rmse: 0.902073\n",
            "[1850]\ttraining's rmse: 0.691616\tvalid_1's rmse: 0.901992\n",
            "[1875]\ttraining's rmse: 0.690951\tvalid_1's rmse: 0.901952\n",
            "[1900]\ttraining's rmse: 0.690272\tvalid_1's rmse: 0.90184\n",
            "[1925]\ttraining's rmse: 0.689493\tvalid_1's rmse: 0.901728\n",
            "[1950]\ttraining's rmse: 0.688626\tvalid_1's rmse: 0.901651\n",
            "[1975]\ttraining's rmse: 0.687946\tvalid_1's rmse: 0.90154\n",
            "[2000]\ttraining's rmse: 0.687356\tvalid_1's rmse: 0.90142\n",
            "[2025]\ttraining's rmse: 0.686642\tvalid_1's rmse: 0.901377\n",
            "[2050]\ttraining's rmse: 0.685757\tvalid_1's rmse: 0.901195\n",
            "[2075]\ttraining's rmse: 0.685125\tvalid_1's rmse: 0.90113\n",
            "[2100]\ttraining's rmse: 0.684489\tvalid_1's rmse: 0.901056\n",
            "[2125]\ttraining's rmse: 0.683696\tvalid_1's rmse: 0.900871\n",
            "[2150]\ttraining's rmse: 0.682971\tvalid_1's rmse: 0.900799\n",
            "[2175]\ttraining's rmse: 0.682418\tvalid_1's rmse: 0.900723\n",
            "[2200]\ttraining's rmse: 0.681661\tvalid_1's rmse: 0.900629\n",
            "[2225]\ttraining's rmse: 0.680944\tvalid_1's rmse: 0.900522\n",
            "[2250]\ttraining's rmse: 0.680348\tvalid_1's rmse: 0.900436\n",
            "[2275]\ttraining's rmse: 0.679771\tvalid_1's rmse: 0.900375\n",
            "[2300]\ttraining's rmse: 0.679194\tvalid_1's rmse: 0.900301\n",
            "[2325]\ttraining's rmse: 0.678506\tvalid_1's rmse: 0.900207\n",
            "[2350]\ttraining's rmse: 0.677967\tvalid_1's rmse: 0.900141\n",
            "[2375]\ttraining's rmse: 0.677373\tvalid_1's rmse: 0.900114\n",
            "[2400]\ttraining's rmse: 0.676836\tvalid_1's rmse: 0.900083\n",
            "[2425]\ttraining's rmse: 0.676198\tvalid_1's rmse: 0.899975\n",
            "[2450]\ttraining's rmse: 0.675643\tvalid_1's rmse: 0.899926\n",
            "[2475]\ttraining's rmse: 0.67507\tvalid_1's rmse: 0.899867\n",
            "[2500]\ttraining's rmse: 0.674548\tvalid_1's rmse: 0.89979\n",
            "[2525]\ttraining's rmse: 0.674094\tvalid_1's rmse: 0.899752\n",
            "[2550]\ttraining's rmse: 0.673641\tvalid_1's rmse: 0.899705\n",
            "[2575]\ttraining's rmse: 0.673073\tvalid_1's rmse: 0.899597\n",
            "[2600]\ttraining's rmse: 0.672535\tvalid_1's rmse: 0.89956\n",
            "[2625]\ttraining's rmse: 0.671991\tvalid_1's rmse: 0.899466\n",
            "[2650]\ttraining's rmse: 0.671572\tvalid_1's rmse: 0.899434\n",
            "[2675]\ttraining's rmse: 0.671034\tvalid_1's rmse: 0.899332\n",
            "[2700]\ttraining's rmse: 0.670444\tvalid_1's rmse: 0.899206\n",
            "[2725]\ttraining's rmse: 0.670021\tvalid_1's rmse: 0.89914\n",
            "[2750]\ttraining's rmse: 0.669551\tvalid_1's rmse: 0.8991\n",
            "[2775]\ttraining's rmse: 0.669039\tvalid_1's rmse: 0.899043\n",
            "[2800]\ttraining's rmse: 0.668454\tvalid_1's rmse: 0.898989\n",
            "[2825]\ttraining's rmse: 0.668011\tvalid_1's rmse: 0.898943\n",
            "[2850]\ttraining's rmse: 0.667572\tvalid_1's rmse: 0.898879\n",
            "[2875]\ttraining's rmse: 0.667152\tvalid_1's rmse: 0.898853\n",
            "[2900]\ttraining's rmse: 0.66666\tvalid_1's rmse: 0.898782\n",
            "[2925]\ttraining's rmse: 0.666109\tvalid_1's rmse: 0.898697\n",
            "[2950]\ttraining's rmse: 0.665607\tvalid_1's rmse: 0.898631\n",
            "[2975]\ttraining's rmse: 0.66505\tvalid_1's rmse: 0.898575\n",
            "[3000]\ttraining's rmse: 0.664598\tvalid_1's rmse: 0.898546\n",
            "[3025]\ttraining's rmse: 0.663981\tvalid_1's rmse: 0.898531\n",
            "[3050]\ttraining's rmse: 0.663619\tvalid_1's rmse: 0.898518\n",
            "[3075]\ttraining's rmse: 0.663233\tvalid_1's rmse: 0.898488\n",
            "[3100]\ttraining's rmse: 0.662725\tvalid_1's rmse: 0.898427\n",
            "[3125]\ttraining's rmse: 0.662182\tvalid_1's rmse: 0.898346\n",
            "[3150]\ttraining's rmse: 0.661628\tvalid_1's rmse: 0.898317\n",
            "[3175]\ttraining's rmse: 0.66119\tvalid_1's rmse: 0.89828\n",
            "[3200]\ttraining's rmse: 0.660817\tvalid_1's rmse: 0.898236\n",
            "[3225]\ttraining's rmse: 0.660331\tvalid_1's rmse: 0.898148\n",
            "[3250]\ttraining's rmse: 0.659942\tvalid_1's rmse: 0.898097\n",
            "[3275]\ttraining's rmse: 0.659515\tvalid_1's rmse: 0.898071\n",
            "[3300]\ttraining's rmse: 0.659001\tvalid_1's rmse: 0.898017\n",
            "[3325]\ttraining's rmse: 0.658419\tvalid_1's rmse: 0.897996\n",
            "[3350]\ttraining's rmse: 0.657972\tvalid_1's rmse: 0.897972\n",
            "[3375]\ttraining's rmse: 0.657461\tvalid_1's rmse: 0.897929\n",
            "[3400]\ttraining's rmse: 0.657088\tvalid_1's rmse: 0.897888\n",
            "[3425]\ttraining's rmse: 0.656662\tvalid_1's rmse: 0.89785\n",
            "[3450]\ttraining's rmse: 0.656268\tvalid_1's rmse: 0.897801\n",
            "[3475]\ttraining's rmse: 0.655762\tvalid_1's rmse: 0.897756\n",
            "[3500]\ttraining's rmse: 0.655242\tvalid_1's rmse: 0.897695\n",
            "[3525]\ttraining's rmse: 0.654923\tvalid_1's rmse: 0.897681\n",
            "[3550]\ttraining's rmse: 0.654329\tvalid_1's rmse: 0.897649\n",
            "[3575]\ttraining's rmse: 0.653897\tvalid_1's rmse: 0.897623\n",
            "[3600]\ttraining's rmse: 0.653449\tvalid_1's rmse: 0.897596\n",
            "[3625]\ttraining's rmse: 0.653103\tvalid_1's rmse: 0.897583\n",
            "[3650]\ttraining's rmse: 0.652691\tvalid_1's rmse: 0.89755\n",
            "[3675]\ttraining's rmse: 0.652275\tvalid_1's rmse: 0.897513\n",
            "[3700]\ttraining's rmse: 0.651944\tvalid_1's rmse: 0.897486\n",
            "[3725]\ttraining's rmse: 0.651538\tvalid_1's rmse: 0.897444\n",
            "[3750]\ttraining's rmse: 0.651013\tvalid_1's rmse: 0.897422\n",
            "[3775]\ttraining's rmse: 0.650626\tvalid_1's rmse: 0.897416\n",
            "[3800]\ttraining's rmse: 0.650183\tvalid_1's rmse: 0.897371\n",
            "[3825]\ttraining's rmse: 0.649832\tvalid_1's rmse: 0.897363\n",
            "[3850]\ttraining's rmse: 0.649463\tvalid_1's rmse: 0.897361\n",
            "[3875]\ttraining's rmse: 0.649041\tvalid_1's rmse: 0.897318\n",
            "[3900]\ttraining's rmse: 0.648705\tvalid_1's rmse: 0.897304\n",
            "[3925]\ttraining's rmse: 0.648283\tvalid_1's rmse: 0.897309\n",
            "[3950]\ttraining's rmse: 0.647912\tvalid_1's rmse: 0.897302\n",
            "[3975]\ttraining's rmse: 0.647412\tvalid_1's rmse: 0.897245\n",
            "[4000]\ttraining's rmse: 0.647049\tvalid_1's rmse: 0.897217\n",
            "[4025]\ttraining's rmse: 0.646637\tvalid_1's rmse: 0.897186\n",
            "[4050]\ttraining's rmse: 0.646186\tvalid_1's rmse: 0.897166\n",
            "[4075]\ttraining's rmse: 0.64579\tvalid_1's rmse: 0.897109\n",
            "[4100]\ttraining's rmse: 0.645393\tvalid_1's rmse: 0.897115\n",
            "[4125]\ttraining's rmse: 0.645054\tvalid_1's rmse: 0.89707\n",
            "[4150]\ttraining's rmse: 0.644568\tvalid_1's rmse: 0.897032\n",
            "[4175]\ttraining's rmse: 0.644131\tvalid_1's rmse: 0.896991\n",
            "[4200]\ttraining's rmse: 0.643614\tvalid_1's rmse: 0.896961\n",
            "[4225]\ttraining's rmse: 0.643312\tvalid_1's rmse: 0.89696\n",
            "[4250]\ttraining's rmse: 0.642986\tvalid_1's rmse: 0.89688\n",
            "[4275]\ttraining's rmse: 0.642588\tvalid_1's rmse: 0.896842\n",
            "[4300]\ttraining's rmse: 0.642262\tvalid_1's rmse: 0.896819\n",
            "[4325]\ttraining's rmse: 0.641957\tvalid_1's rmse: 0.896795\n",
            "[4350]\ttraining's rmse: 0.641563\tvalid_1's rmse: 0.896799\n",
            "[4375]\ttraining's rmse: 0.641284\tvalid_1's rmse: 0.896804\n",
            "[4400]\ttraining's rmse: 0.64093\tvalid_1's rmse: 0.896789\n",
            "[4425]\ttraining's rmse: 0.640626\tvalid_1's rmse: 0.896751\n",
            "[4450]\ttraining's rmse: 0.640271\tvalid_1's rmse: 0.896707\n",
            "[4475]\ttraining's rmse: 0.639931\tvalid_1's rmse: 0.896716\n",
            "[4500]\ttraining's rmse: 0.639497\tvalid_1's rmse: 0.896678\n",
            "[4525]\ttraining's rmse: 0.639186\tvalid_1's rmse: 0.896626\n",
            "[4550]\ttraining's rmse: 0.638887\tvalid_1's rmse: 0.896633\n",
            "[4575]\ttraining's rmse: 0.638613\tvalid_1's rmse: 0.896637\n",
            "[4600]\ttraining's rmse: 0.638274\tvalid_1's rmse: 0.896597\n",
            "[4625]\ttraining's rmse: 0.63788\tvalid_1's rmse: 0.896592\n",
            "[4650]\ttraining's rmse: 0.637485\tvalid_1's rmse: 0.896556\n",
            "[4675]\ttraining's rmse: 0.637201\tvalid_1's rmse: 0.896545\n",
            "[4700]\ttraining's rmse: 0.636911\tvalid_1's rmse: 0.896542\n",
            "[4725]\ttraining's rmse: 0.636521\tvalid_1's rmse: 0.896498\n",
            "[4750]\ttraining's rmse: 0.636204\tvalid_1's rmse: 0.896475\n",
            "[4775]\ttraining's rmse: 0.635897\tvalid_1's rmse: 0.89647\n",
            "[4800]\ttraining's rmse: 0.635591\tvalid_1's rmse: 0.896481\n",
            "[4825]\ttraining's rmse: 0.635304\tvalid_1's rmse: 0.896452\n",
            "[4850]\ttraining's rmse: 0.635012\tvalid_1's rmse: 0.896429\n",
            "[4875]\ttraining's rmse: 0.634562\tvalid_1's rmse: 0.896418\n",
            "[4900]\ttraining's rmse: 0.634233\tvalid_1's rmse: 0.896409\n",
            "[4925]\ttraining's rmse: 0.633775\tvalid_1's rmse: 0.896368\n",
            "[4950]\ttraining's rmse: 0.633492\tvalid_1's rmse: 0.896334\n",
            "[4975]\ttraining's rmse: 0.633133\tvalid_1's rmse: 0.896335\n",
            "[5000]\ttraining's rmse: 0.632686\tvalid_1's rmse: 0.896301\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[5000]\ttraining's rmse: 0.632686\tvalid_1's rmse: 0.896301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
            "  warnings.warn('Using categorical_feature in Dataset.')\n",
            "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
            "  warnings.warn('categorical_feature in param dict is overridden.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[25]\ttraining's rmse: 1.15837\tvalid_1's rmse: 0.997724\n",
            "[50]\ttraining's rmse: 1.08397\tvalid_1's rmse: 0.937942\n",
            "[75]\ttraining's rmse: 1.02482\tvalid_1's rmse: 0.892973\n",
            "[100]\ttraining's rmse: 0.979376\tvalid_1's rmse: 0.860864\n",
            "[125]\ttraining's rmse: 0.944714\tvalid_1's rmse: 0.837844\n",
            "[150]\ttraining's rmse: 0.917969\tvalid_1's rmse: 0.821445\n",
            "[175]\ttraining's rmse: 0.896446\tvalid_1's rmse: 0.809461\n",
            "[200]\ttraining's rmse: 0.87945\tvalid_1's rmse: 0.801104\n",
            "[225]\ttraining's rmse: 0.865565\tvalid_1's rmse: 0.795735\n",
            "[250]\ttraining's rmse: 0.854042\tvalid_1's rmse: 0.791572\n",
            "[275]\ttraining's rmse: 0.844497\tvalid_1's rmse: 0.788574\n",
            "[300]\ttraining's rmse: 0.836165\tvalid_1's rmse: 0.786875\n",
            "[325]\ttraining's rmse: 0.829203\tvalid_1's rmse: 0.785448\n",
            "[350]\ttraining's rmse: 0.822844\tvalid_1's rmse: 0.784418\n",
            "[375]\ttraining's rmse: 0.817349\tvalid_1's rmse: 0.783787\n",
            "[400]\ttraining's rmse: 0.812295\tvalid_1's rmse: 0.783246\n",
            "[425]\ttraining's rmse: 0.807629\tvalid_1's rmse: 0.782805\n",
            "[450]\ttraining's rmse: 0.803053\tvalid_1's rmse: 0.782443\n",
            "[475]\ttraining's rmse: 0.799235\tvalid_1's rmse: 0.782236\n",
            "[500]\ttraining's rmse: 0.795505\tvalid_1's rmse: 0.781938\n",
            "[525]\ttraining's rmse: 0.792061\tvalid_1's rmse: 0.781714\n",
            "[550]\ttraining's rmse: 0.78888\tvalid_1's rmse: 0.781592\n",
            "[575]\ttraining's rmse: 0.786005\tvalid_1's rmse: 0.781589\n",
            "[600]\ttraining's rmse: 0.783275\tvalid_1's rmse: 0.781399\n",
            "[625]\ttraining's rmse: 0.780738\tvalid_1's rmse: 0.781263\n",
            "[650]\ttraining's rmse: 0.778226\tvalid_1's rmse: 0.781041\n",
            "[675]\ttraining's rmse: 0.775931\tvalid_1's rmse: 0.780877\n",
            "[700]\ttraining's rmse: 0.773508\tvalid_1's rmse: 0.780713\n",
            "[725]\ttraining's rmse: 0.771421\tvalid_1's rmse: 0.780659\n",
            "[750]\ttraining's rmse: 0.769532\tvalid_1's rmse: 0.78057\n",
            "[775]\ttraining's rmse: 0.767604\tvalid_1's rmse: 0.780617\n",
            "[800]\ttraining's rmse: 0.765555\tvalid_1's rmse: 0.780525\n",
            "[825]\ttraining's rmse: 0.763731\tvalid_1's rmse: 0.780522\n",
            "[850]\ttraining's rmse: 0.761976\tvalid_1's rmse: 0.780494\n",
            "[875]\ttraining's rmse: 0.760365\tvalid_1's rmse: 0.780549\n",
            "[900]\ttraining's rmse: 0.758793\tvalid_1's rmse: 0.780615\n",
            "[925]\ttraining's rmse: 0.757295\tvalid_1's rmse: 0.780613\n",
            "[950]\ttraining's rmse: 0.755763\tvalid_1's rmse: 0.780494\n",
            "Early stopping, best iteration is:\n",
            "[857]\ttraining's rmse: 0.761508\tvalid_1's rmse: 0.780469\n",
            "\n",
            "xxxxxxxxx 0.7181685780442969 0.31197184326642885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2611.5011653900146"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOV_0vGHz2sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFBcUo4UJMaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/ML Project/Dataset/Original_Source/test.csv')\n",
        "# # submission=pd.DataFrame\n",
        "# submission=pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/ML Project/Dataset/Original_Source/submission (2).csv')\n",
        "\n",
        "# for i in submission:\n",
        "#   print(i)\n",
        "\n",
        "# # for i in range(214200):\n",
        "# #   if i in good_pairs['ID']:\n",
        "# #     df1=pd.DataFrame({\n",
        "# #         \"ID\": i, \n",
        "# #         \"item_cnt_month\": [y_final_test[i]]})\n",
        "# #     submission.append(df1)\n",
        "# #   else:\n",
        "# #     df1=pd.DataFrame({\"ID\": i, \n",
        "# #     \"item_cnt_month\": [0]})\n",
        "# #     submission.append(df1)\n",
        "# # print(submission)\n",
        "# \n",
        "submission = pd.DataFrame({\n",
        "    \"ID\": test_df.index, \n",
        "    \"item_cnt_month\": y_final_test\n",
        "})\n",
        "submission.to_csv('final9.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ySz6eFro2ML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRUow4e_LsLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}